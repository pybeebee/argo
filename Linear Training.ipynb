{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[11  2  2  0  0  0  0]\n",
      " [ 1  6  0  0  0  0  0]\n",
      " [ 1  1 18  0  0  0  0]\n",
      " [ 1  0  0  7  1  2  0]\n",
      " [ 0  0  1  0 27  0  0]\n",
      " [ 5  0  0  3  0  6  1]\n",
      " [ 0  1  0  1  0  1 32]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['linear7.pkl',\n",
       " 'linear7.pkl_01.npy',\n",
       " 'linear7.pkl_02.npy',\n",
       " 'linear7.pkl_03.npy',\n",
       " 'linear7.pkl_04.npy',\n",
       " 'linear7.pkl_05.npy',\n",
       " 'linear7.pkl_06.npy',\n",
       " 'linear7.pkl_07.npy',\n",
       " 'linear7.pkl_08.npy',\n",
       " 'linear7.pkl_09.npy',\n",
       " 'linear7.pkl_10.npy',\n",
       " 'linear7.pkl_11.npy']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# WORKING VERSION OF COMBINED DRAFTS\n",
    "# Create 327x3 matrix, for each image set contains neutral image name, expression image name, emotion label\n",
    "# Generate normalized Euclidean Differences array\n",
    "# Generate target values array\n",
    "# Feed to SVM, evaulate with Cross Validation\n",
    "import numpy as np\n",
    "import cv2\n",
    "import dlib\n",
    "import os\n",
    "import math\n",
    "from sklearn import svm\n",
    "from sklearn import cross_validation\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "# Function that returns the distance between two given points\n",
    "def ptDist(pt1, pt2):\n",
    "    x1 = pt1[0]\n",
    "    x2 = pt2[0]\n",
    "    y1 = pt1[1]\n",
    "    y2 = pt2[1]\n",
    "    return math.sqrt((x1-x2)*(x1-x2) + (y1-y2)*(y1-y2))\n",
    "\n",
    "# Function that returns array of 68 landmarks for given image name\n",
    "def get68LMA(imgName):\n",
    "    predictor_path = \"/Users/kaili/Dropbox/HSSF1516/shape_predictor_68_face_landmarks.dat\"\n",
    "    detector = dlib.get_frontal_face_detector()\n",
    "    predictor = dlib.shape_predictor(predictor_path)\n",
    "    LMA = []\n",
    "    image = cv2.imread(imgName, 1)\n",
    "    \n",
    "    # Detect face(s)\n",
    "    dets = detector(image, 1)\n",
    "\n",
    "    # Get list of coordinates of 68 facial landmarks for each face\n",
    "    for d in dets:\n",
    "        shape = predictor(image, d)\n",
    "        for index in range(0, 68):\n",
    "            point = shape.part(index)\n",
    "            tuplepoint = (point.x, point.y)\n",
    "            LMA.append(tuplepoint)\n",
    "    return LMA\n",
    "\n",
    "# Function that eturns normalized array of Euclidean distances given two arrays of coordinates\n",
    "def getEucDist(nLA, eLA):\n",
    "    LDA = []\n",
    "    sum = 0\n",
    "    # Get array of distances\n",
    "    for pt in range(68):\n",
    "        dif = ptDist(eLA[pt], nLA[pt])     \n",
    "        LDA.append(dif)\n",
    "        sum += dif\n",
    "    avg = sum/68\n",
    "    # Normalize array\n",
    "    for i in range(68):\n",
    "        LDA[i] -= avg      \n",
    "    return LDA\n",
    "\n",
    "# Function that returns 13-point numpy vector of distances between selected physiological pts, given the 68-point vector\n",
    "def getPDA(iLA):\n",
    "    PDA = []\n",
    "    PDA.append(ptDist(iLA[21], iLA[22])) #distance 0: distance between brows\n",
    "    PDA.append(ptDist(iLA[21], iLA[27])) #distance 1: inner brow to nose- left\n",
    "    PDA.append(ptDist(iLA[22], iLA[27])) #distance 2: inner brow to nose- right\n",
    "    PDA.append(ptDist(iLA[21], iLA[39])) #distance 3: inner brow to inner eye corner- left\n",
    "    PDA.append(ptDist(iLA[22], iLA[42])) #distance 4: inner brow to inner eye corner- right\n",
    "    PDA.append(ptDist(iLA[36], iLA[48])) #distance 5: outer eye corner to outer mouth corner- left\n",
    "    PDA.append(ptDist(iLA[45], iLA[54])) #distance 6: outer eye corner to outer mouth corner- right\n",
    "    PDA.append(ptDist(iLA[48], iLA[54])) #distance 7: distance between mouth corners\n",
    "    PDA.append(ptDist(iLA[62], iLA[66])) #distance 8: height of mouth\n",
    "    PDA.append(ptDist(iLA[36], iLA[50])) #distance 9: raising of upper lip\n",
    "    PDA.append(ptDist(iLA[45], iLA[52])) #distance 10: raising of upper lip\n",
    "    PDA.append(ptDist(iLA[19], iLA[37])) #distance 11: brow arch- left\n",
    "    PDA.append(ptDist(iLA[24], iLA[44])) #distance 12: brow arch- right\n",
    "    return np.asarray(PDA)\n",
    "\n",
    "\"\"\"MAIN PROGRAM BEGINS\"\"\"\n",
    "# Get paths to face images and emotion labels\n",
    "imagedir = []\n",
    "labeldir = []\n",
    "root, dirs, files = os.walk(\"./cohn-kanade-images2\",).__next__()\n",
    "for d in dirs:\n",
    "    imagedir.append(os.path.join(root,d))\n",
    "    labeldir.append(os.path.join('./Emotion',d))\n",
    "\n",
    "# initialize variables\n",
    "dataset = []\n",
    "NLA = []\n",
    "ELA = []\n",
    "\n",
    "# look over all directories of individuals S001 through S999\n",
    "for d in dirs:\n",
    "    for dd in os.listdir(os.path.join('./cohn-kanade-images2',d)):\n",
    "        # skip over .DS_Store files\n",
    "        if 'Store' not in dd:\n",
    "            curdir = os.path.join('./cohn-kanade-images2',d,dd)\n",
    "            files = os.listdir(curdir)\n",
    "            baseimage = os.path.join(curdir,files[0])\n",
    "            faceimage = os.path.join(curdir,files[-1])\n",
    "\n",
    "            # get directories of multiple expressions of same person\n",
    "            labeldir = os.path.join('./Emotion',d,dd)\n",
    "            if os.path.isdir(labeldir):\n",
    "                labels = os.listdir(labeldir)\n",
    "                # read label file if it exists\n",
    "                if len(labels)>0:\n",
    "                    f=open(os.path.join(labeldir,labels[0]),\"r\")\n",
    "                    for line in f:\n",
    "                        targetlabel = float(line)\n",
    "                    f.close()\n",
    "                    \n",
    "                    # append label to dataset\n",
    "                    dataset.append([baseimage, faceimage, targetlabel])\n",
    "\n",
    "\"\"\"68-POINT LANDMARK DETECTION MODEL\"\"\"\n",
    "# Get array of Euclidean differences between neutral and expression 68-landmark arrays\n",
    "tempLDA = []\n",
    "for imgIndex in range(len(dataset)):\n",
    "    nImgName = dataset[imgIndex][0]\n",
    "    eImgName = dataset[imgIndex][1]\n",
    "    nLA = get68LMA(nImgName)\n",
    "    eLA = get68LMA(eImgName)\n",
    "    NLA.append(nLA)\n",
    "    ELA.append(eLA)\n",
    "    tempLDA.append(getEucDist(nLA, eLA))\n",
    "LDA = np.asmatrix(tempLDA)\n",
    "\n",
    "# Create target value array for expression images\n",
    "tempTargetValues = []\n",
    "for imgSet in range(len(dataset)):\n",
    "    tempTargetValues.append(dataset[imgSet][2])\n",
    "targetValues = np.asarray(tempTargetValues)\n",
    "\n",
    "# # Feed LDA and targetValues into SVM, use cross validation, generate confusion matrix\n",
    "# x_train, x_test, y_train, y_test = cross_validation.train_test_split(LDA, targetValues, test_size=0.4, random_state=2)\n",
    "# normCLF = svm.SVC(kernel='linear').fit(x_train, y_train)\n",
    "# predictions = normCLF.predict(x_test)\n",
    "# cmatrix = confusion_matrix(y_test, predictions)\n",
    "\n",
    "# # Format CMatrix to show percentages, rounded to 1 decimal point\n",
    "# np.set_printoptions(precision=1)\n",
    "# rowSums = np.sum(cmatrix, axis=1)\n",
    "# pCMatrix = np.divide(cmatrix*100., rowSums, dtype=float)\n",
    "# print pCMatrix\n",
    "\n",
    "\n",
    "\"\"\"PHYSIOLOGICALLY BASED LANDMARK DETECTION MODEL\"\"\"\n",
    "# Get array of corresponding differences between 13 distances of base images and expression images\n",
    "tempPDA = []\n",
    "for i in range(len(NLA)):\n",
    "    tempPDA.append(getPDA(NLA[i])-getPDA(ELA[i]))\n",
    "PDA = np.asmatrix(tempPDA)\n",
    "\n",
    "# Feed PDA and targetValues into SVM, use cross validation, generate confusion matrix\n",
    "x_train2, x_test2, y_train2, y_test2 = cross_validation.train_test_split(PDA, targetValues, test_size=0.4, random_state=3)\n",
    "linCLF = svm.SVC(kernel='linear',probability=True).fit(x_train2, y_train2)\n",
    "predictions2 = linCLF.predict(x_test2)\n",
    "cMatrix2 = confusion_matrix(y_test2, predictions2)\n",
    "\n",
    "# Format CMatrix2 to show percentages, already rounded to 1 decimal point\n",
    "# rowSums2 = np.sum(cMatrix2, axis=1)\n",
    "# pCMatrix2 = np.divide(cMatrix2*100., rowSums2, dtype=float)\n",
    "print(cMatrix2)\n",
    "\n",
    "joblib.dump(linCLF, 'linear7.pkl') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open(\"/Users/kaili/Dropbox/HSSF1617/NeutralLMLocs.pkl\", 'wb') as f:\n",
    "    pickle.dump(NLA, f)\n",
    "f.close()\n",
    "with open(\"/Users/kaili/Dropbox/HSSF1617/ExpLMLocs.pkl\", 'wb') as f:\n",
    "    pickle.dump(ELA, f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(21, 13)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linCLF.coef_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[11  4  0  0  0  0  0]\n",
      " [ 1  6  0  0  0  0  0]\n",
      " [ 4  0 16  0  0  0  0]\n",
      " [ 1  0  0  6  1  2  1]\n",
      " [ 0  1  0  0 27  0  0]\n",
      " [ 6  0  1  2  0  6  0]\n",
      " [ 0  0  0  0  0  0 35]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['workingLinearSVM.pkl',\n",
       " 'workingLinearSVM.pkl_01.npy',\n",
       " 'workingLinearSVM.pkl_02.npy',\n",
       " 'workingLinearSVM.pkl_03.npy']"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def getPDA(iLA):\n",
    "    PDA = []\n",
    "    PDA.append(ptDist(iLA[21], iLA[22])) #distance 0: distance between brows\n",
    "    PDA.append(ptDist(iLA[21], iLA[27])) #distance 1: inner brow to nose- left\n",
    "    PDA.append(ptDist(iLA[22], iLA[27])) #distance 2: inner brow to nose- right\n",
    "    PDA.append(ptDist(iLA[21], iLA[39])) #distance 3: inner brow to inner eye corner- left\n",
    "    PDA.append(ptDist(iLA[22], iLA[42])) #distance 4: inner brow to inner eye corner- right\n",
    "    PDA.append(ptDist(iLA[36], iLA[48])) #distance 5: outer eye corner to outer mouth corner- left\n",
    "    PDA.append(ptDist(iLA[45], iLA[54])) #distance 6: outer eye corner to outer mouth corner- right\n",
    "    PDA.append(ptDist(iLA[48], iLA[54])) #distance 7: distance between mouth corners\n",
    "    PDA.append(ptDist(iLA[62], iLA[66])) #distance 8: height of mouth\n",
    "    PDA.append(ptDist(iLA[36], iLA[50])) #distance 9: raising of upper lip\n",
    "    PDA.append(ptDist(iLA[45], iLA[52])) #distance 10: raising of upper lip\n",
    "    PDA.append(ptDist(iLA[19], iLA[37])) #distance 11: brow arch- left\n",
    "    PDA.append(ptDist(iLA[24], iLA[44])) #distance 12: brow arch- right\n",
    "    return np.asarray(PDA)\n",
    "\n",
    "tempPDA = []\n",
    "for i in range(len(NLA)):\n",
    "    tempPDA.append(getPDA(NLA[i])-getPDA(ELA[i]))\n",
    "PDA = np.asmatrix(tempPDA)\n",
    "\n",
    "linCLF = svm.LinearSVC().fit(x_train2, y_train2)\n",
    "predictions2 = linCLF.predict(x_test2)\n",
    "cMatrix2 = confusion_matrix(y_test2, predictions2)\n",
    "print(cMatrix2)\n",
    "\n",
    "joblib.dump(linCLF, 'workingLinearSVM.pkl') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[11  2  1  0  0  0  1]\n",
      " [ 0  7  0  0  0  0  0]\n",
      " [ 1  0 19  0  0  0  0]\n",
      " [ 1  1  0  7  1  1  0]\n",
      " [ 0  0  1  0 27  0  0]\n",
      " [ 1  5  0  2  1  6  0]\n",
      " [ 0  2  0  2  0  0 31]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.externals import joblib\n",
    "from sklearn import svm\n",
    "\n",
    "\n",
    "loaded = joblib.load('theModel.pkl')\n",
    "predictionsasdfsdf = loaded.predict(x_test2)\n",
    "print(confusion_matrix(y_test2, predictionsasdfsdf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['workingLinearSVM.pkl',\n",
       " 'workingLinearSVM.pkl_01.npy',\n",
       " 'workingLinearSVM.pkl_02.npy',\n",
       " 'workingLinearSVM.pkl_03.npy']"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(linCLF, 'workingLinearSVM.pkl') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[-13.03770501,  -2.54412371,  -3.23733987,   1.60101977,\n",
       "           2.58383776,  20.25504484,  21.54680737, -37.02862209,\n",
       "         -16.        ,   7.42182001,  12.26044002,  -4.04097145,\n",
       "          -2.95473831]])"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PDA[13]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./cohn-kanade-images2/S014/002/S014_002_00000016.png'"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[11][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 5.])"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linCLF.predict(np.asarray([2,-1,4,-2,5,2,19,14,-19,-11,9,-3,-2]).reshape(1,-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[ 2.994,  3.399,  1.664,  0.267, -0.539, -2.595, -0.717,  2.995,\n",
       "         -1.   ,  4.965,  6.064,  2.046,  2.043]])"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PDA[11]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 5.])"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#emo = [2, -1,    4, -2,   5,  2,  19, 14, -19, -11, 9, -3, -2]\n",
    "#emo = [0, -27, -23, -29, -28, 28, 12, -5, 0, -15, -11, -4, -6]\n",
    "emo =  [0, -2,    5, 0,   0,  0,  12, 0, -19, -11, 0, -4, -6]\n",
    "linCLF.predict(np.asarray(emo).reshape(1,-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.349 -0.331  0.37  -0.118 -0.071 -0.238 -0.011  0.046  0.069  0.015\n",
      "  0.035  0.061 -0.008]\n"
     ]
    }
   ],
   "source": [
    "np.set_printoptions(precision=3)\n",
    "np.set_printoptions(suppress=True)\n",
    "\n",
    "print(linCLF.coef_[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 6.])"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#emo = [3, 3,   2,   0,   -1, -3,  -1,  3, -1, 5,    6,  2,  2]\n",
    "#emo = [5, -32, -28, -34, -33, -15, -31, 3, 1, -14, -12, -2, -3]\n",
    "emo = [5, -32, -28, -34, -33, -15, -31, 0, 0, 0, 0, 0, 0]\n",
    "linCLF.predict(np.asarray(emo).reshape(1,-1)/5)\n",
    "\n",
    "\n",
    "## sadness = first three displacements most important\n",
    "#### make a transformation matrix to scale first 7 displacements by 1/5 and set last 6 to 0 since they never change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 6.])"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#####\n",
    "\n",
    "\n",
    "# SUCCESSFUL SADNESS TRANSFORMATION\n",
    "\n",
    "\n",
    "#####\n",
    "\n",
    "TRANS_MAT = np.asmatrix([[.2,0,0,0,0,0,0,0,0,0,0,0,0], \n",
    "             [0,.2,0,0,0,0,0,0,0,0,0,0,0], \n",
    "             [0,0,.2,0,0,0,0,0,0,0,0,0,0], \n",
    "             [0,0,0,0,.2,0,0,0,0,0,0,0,0], \n",
    "             [0,0,0,0,0,.2,0,0,0,0,0,0,0], \n",
    "             [0,0,0,0,0,0,.2,0,0,0,0,0,0], \n",
    "             [0,0,0,0,0,0,0,.2,0,0,0,0,0], \n",
    "             [0,0,0,0,0,0,0,0,0,0,0,0,0], \n",
    "             [0,0,0,0,0,0,0,0,0,0,0,0,0], \n",
    "             [0,0,0,0,0,0,0,0,0,0,0,0,0],\n",
    "             [0,0,0,0,0,0,0,0,0,0,0,0,0], \n",
    "             [0,0,0,0,0,0,0,0,0,0,0,0,0], \n",
    "             [0,0,0,0,0,0,0,0,0,0,0,0,0]])\n",
    "ARGO_EXP = np.asmatrix([5, -32, -28, -34, -33, -15, -31, 3, 1, -14, -12, -2, -3])\n",
    "linCLF.predict(np.asarray(ARGO_EXP*TRANS_MAT).reshape(1,-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 2.])"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ARGO_EXP2 = [-11, -12, -9, -5, -7, 28, 25, -3, 2, 5, 9, 2, 5]\n",
    "linCLF.predict(np.asarray(ARGO_EXP2*TRANS_MAT).reshape(1,-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
