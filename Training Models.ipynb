{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Classify CK+ moderate images into ModEm folder\n",
    "import os\n",
    "\n",
    "for root, dirs, files in os.walk(\"./cohn-kanade-images2\"):\n",
    "    filelist=[]\n",
    "    for name in files:\n",
    "        filename = os.path.join(root, name)\n",
    "        if '.png' in filename:\n",
    "            filelist.append(filename)\n",
    "    if len(filelist) > 0:\n",
    "        modfile = filelist[int(len(filelist)/2)]\n",
    "        !cp $modfile ModEm\n",
    "        print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Classify CK+ Neutral and Expression images into respective folders\n",
    "import os\n",
    "for root, dirs, files in os.walk(\"./cohn-kanade-images2\"):\n",
    "    filelist=[]\n",
    "    for name in files:\n",
    "        filename = os.path.join(root, name)\n",
    "        if '.png' in filename:\n",
    "            filelist.append(filename)\n",
    "    if len(filelist) > 0:\n",
    "        neutralfile = filelist[0]\n",
    "        !cp $neutralfile Neutral\n",
    "        expressfile = filelist[-1]\n",
    "        !cp $expressfile StrEm\n",
    "        print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[11  2  1  0  0  0  1]\n",
      " [ 0  7  0  0  0  0  0]\n",
      " [ 1  0 19  0  0  0  0]\n",
      " [ 1  1  0  7  1  1  0]\n",
      " [ 0  0  1  0 27  0  0]\n",
      " [ 1  5  0  2  1  6  0]\n",
      " [ 0  2  0  2  0  0 31]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['poly7.pkl',\n",
       " 'poly7.pkl_01.npy',\n",
       " 'poly7.pkl_02.npy',\n",
       " 'poly7.pkl_03.npy',\n",
       " 'poly7.pkl_04.npy',\n",
       " 'poly7.pkl_05.npy',\n",
       " 'poly7.pkl_06.npy',\n",
       " 'poly7.pkl_07.npy',\n",
       " 'poly7.pkl_08.npy',\n",
       " 'poly7.pkl_09.npy',\n",
       " 'poly7.pkl_10.npy',\n",
       " 'poly7.pkl_11.npy']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# WORKING VERSION OF COMBINED DRAFTS\n",
    "# Create 327x3 matrix, for each image set contains neutral image name, expression image name, emotion label\n",
    "# Generate normalized Euclidean Differences array\n",
    "# Generate target values array\n",
    "# Feed to SVM, evaulate with Cross Validation\n",
    "import numpy as np\n",
    "import cv2\n",
    "import dlib\n",
    "import os\n",
    "import math\n",
    "from sklearn import svm\n",
    "from sklearn import cross_validation\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "# Function that returns the distance between two given points\n",
    "def ptDist(pt1, pt2):\n",
    "    x1 = pt1[0]\n",
    "    x2 = pt2[0]\n",
    "    y1 = pt1[1]\n",
    "    y2 = pt2[1]\n",
    "    return math.sqrt((x1-x2)*(x1-x2) + (y1-y2)*(y1-y2))\n",
    "\n",
    "# Function that returns array of 68 landmarks for given image name\n",
    "def get68LMA(imgName):\n",
    "    predictor_path = \"/Users/kaili/Dropbox/HSSF1516/shape_predictor_68_face_landmarks.dat\"\n",
    "    detector = dlib.get_frontal_face_detector()\n",
    "    predictor = dlib.shape_predictor(predictor_path)\n",
    "    LMA = []\n",
    "    image = cv2.imread(imgName, 1)\n",
    "    \n",
    "    # Detect face(s)\n",
    "    dets = detector(image, 1)\n",
    "\n",
    "    # Get list of coordinates of 68 facial landmarks for each face\n",
    "    for d in dets:\n",
    "        shape = predictor(image, d)\n",
    "        for index in range(0, 68):\n",
    "            point = shape.part(index)\n",
    "            tuplepoint = (point.x, point.y)\n",
    "            LMA.append(tuplepoint)\n",
    "    return LMA\n",
    "\n",
    "# Function that eturns normalized array of Euclidean distances given two arrays of coordinates\n",
    "def getEucDist(nLA, eLA):\n",
    "    LDA = []\n",
    "    sum = 0\n",
    "    # Get array of distances\n",
    "    for pt in range(68):\n",
    "        dif = ptDist(eLA[pt], nLA[pt])     \n",
    "        LDA.append(dif)\n",
    "        sum += dif\n",
    "    avg = sum/68\n",
    "    # Normalize array\n",
    "    for i in range(68):\n",
    "        LDA[i] -= avg      \n",
    "    return LDA\n",
    "\n",
    "# Function that returns 13-point numpy vector of distances between selected physiological pts, given the 68-point vector\n",
    "def getPDA(iLA):\n",
    "    PDA = []\n",
    "    PDA.append(ptDist(iLA[20], iLA[21])) #distance 0: distance between brows\n",
    "    PDA.append(ptDist(iLA[20], iLA[26])) #distance 1: inner brow to nose- left\n",
    "    PDA.append(ptDist(iLA[21], iLA[26])) #distance 2: inner brow to nose- right\n",
    "    PDA.append(ptDist(iLA[20], iLA[38])) #distance 3: inner brow to inner eye corner- left\n",
    "    PDA.append(ptDist(iLA[21], iLA[41])) #distance 4: inner brow to inner eye corner- right\n",
    "    PDA.append(ptDist(iLA[35], iLA[47])) #distance 5: outer eye corner to outer mouth corner- left\n",
    "    PDA.append(ptDist(iLA[44], iLA[53])) #distance 6: outer eye corner to outer mouth corner- right\n",
    "    PDA.append(ptDist(iLA[47], iLA[53])) #distance 7: distance between mouth corners\n",
    "    PDA.append(ptDist(iLA[61], iLA[65])) #distance 8: height of mouth\n",
    "    PDA.append(ptDist(iLA[35], iLA[49])) #distance 9: raising of upper lip\n",
    "    PDA.append(ptDist(iLA[44], iLA[51])) #distance 10: raising of upper lip\n",
    "    PDA.append(ptDist(iLA[18], iLA[36])) #distance 11: brow arch- left\n",
    "    PDA.append(ptDist(iLA[23], iLA[43])) #distance 12: brow arch- right\n",
    "    return np.asarray(PDA)\n",
    "\n",
    "\"\"\"MAIN PROGRAM BEGINS\"\"\"\n",
    "# Get paths to face images and emotion labels\n",
    "imagedir = []\n",
    "labeldir = []\n",
    "root, dirs, files = os.walk(\"./cohn-kanade-images2\",).__next__()\n",
    "for d in dirs:\n",
    "    imagedir.append(os.path.join(root,d))\n",
    "    labeldir.append(os.path.join('./Emotion',d))\n",
    "\n",
    "# initialize variables\n",
    "dataset = []\n",
    "NLA = []\n",
    "ELA = []\n",
    "\n",
    "# look over all directories of individuals S001 through S999\n",
    "for d in dirs:\n",
    "    for dd in os.listdir(os.path.join('./cohn-kanade-images2',d)):\n",
    "        # skip over .DS_Store files\n",
    "        if 'Store' not in dd:\n",
    "            curdir = os.path.join('./cohn-kanade-images2',d,dd)\n",
    "            files = os.listdir(curdir)\n",
    "            baseimage = os.path.join(curdir,files[0])\n",
    "            faceimage = os.path.join(curdir,files[-1])\n",
    "\n",
    "            # get directories of multiple expressions of same person\n",
    "            labeldir = os.path.join('./Emotion',d,dd)\n",
    "            if os.path.isdir(labeldir):\n",
    "                labels = os.listdir(labeldir)\n",
    "                # read label file if it exists\n",
    "                if len(labels)>0:\n",
    "                    f=open(os.path.join(labeldir,labels[0]),\"r\")\n",
    "                    for line in f:\n",
    "                        targetlabel = float(line)\n",
    "                    f.close()\n",
    "                    \n",
    "                    # append label to dataset\n",
    "                    dataset.append([baseimage, faceimage, targetlabel])\n",
    "\n",
    "\"\"\"68-POINT LANDMARK DETECTION MODEL\"\"\"\n",
    "# Get array of Euclidean differences between neutral and expression 68-landmark arrays\n",
    "tempLDA = []\n",
    "for imgIndex in range(len(dataset)):\n",
    "    nImgName = dataset[imgIndex][0]\n",
    "    eImgName = dataset[imgIndex][1]\n",
    "    nLA = get68LMA(nImgName)\n",
    "    eLA = get68LMA(eImgName)\n",
    "    NLA.append(nLA)\n",
    "    ELA.append(eLA)\n",
    "    tempLDA.append(getEucDist(nLA, eLA))\n",
    "LDA = np.asmatrix(tempLDA)\n",
    "\n",
    "# Create target value array for expression images\n",
    "tempTargetValues = []\n",
    "for imgSet in range(len(dataset)):\n",
    "    tempTargetValues.append(dataset[imgSet][2])\n",
    "targetValues = np.asarray(tempTargetValues)\n",
    "\n",
    "# # Feed LDA and targetValues into SVM, use cross validation, generate confusion matrix\n",
    "# x_train, x_test, y_train, y_test = cross_validation.train_test_split(LDA, targetValues, test_size=0.4, random_state=2)\n",
    "# normCLF = svm.SVC(kernel='linear').fit(x_train, y_train)\n",
    "# predictions = normCLF.predict(x_test)\n",
    "# cmatrix = confusion_matrix(y_test, predictions)\n",
    "\n",
    "# # Format CMatrix to show percentages, rounded to 1 decimal point\n",
    "# np.set_printoptions(precision=1)\n",
    "# rowSums = np.sum(cmatrix, axis=1)\n",
    "# pCMatrix = np.divide(cmatrix*100., rowSums, dtype=float)\n",
    "# print pCMatrix\n",
    "\n",
    "\n",
    "\"\"\"PHYSIOLOGICALLY BASED LANDMARK DETECTION MODEL\"\"\"\n",
    "# Get array of corresponding differences between 13 distances of base images and expression images\n",
    "tempPDA = []\n",
    "for i in range(len(NLA)):\n",
    "    tempPDA.append(getPDA(NLA[i])-getPDA(ELA[i]))\n",
    "PDA = np.asmatrix(tempPDA)\n",
    "np.savetxt('PDAfile', PDA) #Save PDA as human-readable format file\n",
    "\n",
    "# Feed PDA and targetValues into SVM, use cross validation, generate confusion matrix\n",
    "x_train2, x_test2, y_train2, y_test2 = cross_validation.train_test_split(PDA, targetValues, test_size=0.4, random_state=3)\n",
    "physCLF = svm.SVC(kernel='poly',probability=True).fit(x_train2, y_train2)\n",
    "predictions2 = physCLF.predict(x_test2)\n",
    "cMatrix2 = confusion_matrix(y_test2, predictions2)\n",
    "\n",
    "# Format CMatrix2 to show percentages, already rounded to 1 decimal point\n",
    "# rowSums2 = np.sum(cMatrix2, axis=1)\n",
    "# pCMatrix2 = np.divide(cMatrix2*100., rowSums2, dtype=float)\n",
    "print(cMatrix2)\n",
    "\n",
    "joblib.dump(polyCLF, 'poly7.pkl') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# SET UP METHODS FOR SVM\n",
    "\n",
    "import numpy as np\n",
    "import cv2\n",
    "import dlib\n",
    "import os\n",
    "import math\n",
    "from sklearn import svm\n",
    "from sklearn import cross_validation\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Function that returns the distance between two given points\n",
    "def ptDist(pt1, pt2):\n",
    "    x1 = pt1[0]\n",
    "    x2 = pt2[0]\n",
    "    y1 = pt1[1]\n",
    "    y2 = pt2[1]\n",
    "    return math.sqrt((x1-x2)*(x1-x2) + (y1-y2)*(y1-y2))\n",
    "\n",
    "# Function that returns array of 68 landmarks for given image name\n",
    "def get68LMA(imgName):\n",
    "    predictor_path = \"/Users/kaili/Dropbox/HSSF1516/shape_predictor_68_face_landmarks.dat\"\n",
    "    detector = dlib.get_frontal_face_detector()\n",
    "    predictor = dlib.shape_predictor(predictor_path)\n",
    "    LMA = []\n",
    "    image = cv2.imread(imgName, 1)\n",
    "    \n",
    "    # Detect face(s)\n",
    "    dets = detector(image, 1)\n",
    "\n",
    "    # Get list of coordinates of 68 facial landmarks for each face\n",
    "    shape = predictor(image, dets[0])\n",
    "    for index in range(0, 68):\n",
    "        point = shape.part(index)\n",
    "        tuplepoint = (point.x, point.y)\n",
    "        LMA.append(tuplepoint)\n",
    "    return LMA\n",
    "\n",
    "# Function that eturns normalized array of Euclidean distances given two arrays of coordinates\n",
    "def getEucDist(nLA, eLA):\n",
    "    LDA = []\n",
    "    sum = 0\n",
    "    # Get array of distances\n",
    "    for pt in range(68):\n",
    "        dif = ptDist(eLA[pt], nLA[pt])     \n",
    "        LDA.append(dif)\n",
    "        sum += dif\n",
    "    avg = sum/68\n",
    "    # Normalize array\n",
    "    for i in range(68):\n",
    "        LDA[i] -= avg      \n",
    "    return LDA\n",
    "\n",
    "# Function that returns 13-point numpy vector of distances between selected physiological pts, given the 68-point vector\n",
    "def getPhysDA(iLA):\n",
    "    PDA = []\n",
    "    PDA.append(ptDist(iLA[20], iLA[21])) #distance 0: distance between brows\n",
    "    PDA.append(ptDist(iLA[20], iLA[26])) #distance 1: inner brow to nose- left\n",
    "    PDA.append(ptDist(iLA[21], iLA[26])) #distance 2: inner brow to nose- right\n",
    "    PDA.append(ptDist(iLA[20], iLA[38])) #distance 3: inner brow to inner eye corner- left\n",
    "    PDA.append(ptDist(iLA[21], iLA[41])) #distance 4: inner brow to inner eye corner- right\n",
    "    PDA.append(ptDist(iLA[35], iLA[47])) #distance 5: outer eye corner to outer mouth corner- left\n",
    "    PDA.append(ptDist(iLA[44], iLA[53])) #distance 6: outer eye corner to outer mouth corner- right\n",
    "    PDA.append(ptDist(iLA[47], iLA[53])) #distance 7: distance between mouth corners\n",
    "    PDA.append(ptDist(iLA[61], iLA[65])) #distance 8: height of mouth\n",
    "    PDA.append(ptDist(iLA[35], iLA[49])) #distance 9: raising of upper lip\n",
    "    PDA.append(ptDist(iLA[44], iLA[51])) #distance 10: raising of upper lip\n",
    "    PDA.append(ptDist(iLA[18], iLA[36])) #distance 11: brow arch- left\n",
    "    PDA.append(ptDist(iLA[23], iLA[43])) #distance 12: brow arch- right\n",
    "    return np.asarray(PDA)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 7  1  2  0  0  1 31]\n",
      " [ 0  4  0  0  1  2  7]\n",
      " [ 0  0 12  0  0  0 30]\n",
      " [ 0  0  0  0  0  0 18]\n",
      " [ 0  0  1  0 35  0 31]\n",
      " [ 0  0  1  0  0  3 13]\n",
      " [ 0  0  1  0  0  0 61]]\n"
     ]
    }
   ],
   "source": [
    "# Copied from last year\n",
    "#\n",
    "#\n",
    "\n",
    "# WORKING VERSION OF COMBINED DRAFTS\n",
    "# Create 327x3 matrix, for each image set contains neutral image name, expression image name, emotion label\n",
    "# Generate normalized Euclidean Differences array\n",
    "# Generate target values array\n",
    "# Feed to SVM, evaulate with Cross Validation\n",
    "import numpy as np\n",
    "import cv2\n",
    "import dlib\n",
    "import os\n",
    "import math\n",
    "from sklearn import svm\n",
    "from sklearn import cross_validation\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Function that returns the distance between two given points\n",
    "def ptDist(pt1, pt2):\n",
    "    x1 = pt1[0]\n",
    "    x2 = pt2[0]\n",
    "    y1 = pt1[1]\n",
    "    y2 = pt2[1]\n",
    "    return math.sqrt((x1-x2)*(x1-x2) + (y1-y2)*(y1-y2))\n",
    "\n",
    "# Function that returns array of 68 landmarks for given image name\n",
    "def get68LMA(imgName):\n",
    "    predictor_path = \"/Users/kaili/Dropbox/HSSF1516/shape_predictor_68_face_landmarks.dat\"\n",
    "    detector = dlib.get_frontal_face_detector()\n",
    "    predictor = dlib.shape_predictor(predictor_path)\n",
    "    LMA = []\n",
    "    image = cv2.imread(imgName, 1)\n",
    "    \n",
    "    # Detect face(s)\n",
    "    dets = detector(image, 1)\n",
    "\n",
    "    # Get list of coordinates of 68 facial landmarks for each face\n",
    "    for d in dets:\n",
    "        shape = predictor(image, d)\n",
    "        for index in range(0, 68):\n",
    "            point = shape.part(index)\n",
    "            tuplepoint = (point.x, point.y)\n",
    "            LMA.append(tuplepoint)\n",
    "    return LMA\n",
    "\n",
    "# Function that eturns normalized array of Euclidean distances given two arrays of coordinates\n",
    "def getEucDist(nLA, eLA):\n",
    "    LDA = []\n",
    "    sum = 0\n",
    "    # Get array of distances\n",
    "    for pt in range(68):\n",
    "        dif = ptDist(eLA[pt], nLA[pt])     \n",
    "        LDA.append(dif)\n",
    "        sum += dif\n",
    "    avg = sum/68\n",
    "    # Normalize array\n",
    "    for i in range(68):\n",
    "        LDA[i] -= avg      \n",
    "    return LDA\n",
    "\n",
    "# Function that returns 13-point numpy vector of distances between selected physiological pts, given the 68-point vector\n",
    "def getPDA(iLA):\n",
    "    PDA = []\n",
    "    PDA.append(ptDist(iLA[20], iLA[21])) #distance 0: distance between brows\n",
    "    PDA.append(ptDist(iLA[20], iLA[26])) #distance 1: inner brow to nose- left\n",
    "    PDA.append(ptDist(iLA[21], iLA[26])) #distance 2: inner brow to nose- right\n",
    "    PDA.append(ptDist(iLA[20], iLA[38])) #distance 3: inner brow to inner eye corner- left\n",
    "    PDA.append(ptDist(iLA[21], iLA[41])) #distance 4: inner brow to inner eye corner- right\n",
    "    PDA.append(ptDist(iLA[35], iLA[47])) #distance 5: outer eye corner to outer mouth corner- left\n",
    "    PDA.append(ptDist(iLA[44], iLA[53])) #distance 6: outer eye corner to outer mouth corner- right\n",
    "    PDA.append(ptDist(iLA[47], iLA[53])) #distance 7: distance between mouth corners\n",
    "    PDA.append(ptDist(iLA[61], iLA[65])) #distance 8: height of mouth\n",
    "    PDA.append(ptDist(iLA[35], iLA[49])) #distance 9: raising of upper lip\n",
    "    PDA.append(ptDist(iLA[44], iLA[51])) #distance 10: raising of upper lip\n",
    "    PDA.append(ptDist(iLA[18], iLA[36])) #distance 11: brow arch- left\n",
    "    PDA.append(ptDist(iLA[23], iLA[43])) #distance 12: brow arch- right\n",
    "    return np.asarray(PDA)\n",
    "\n",
    "\"\"\"MAIN PROGRAM BEGINS\"\"\"\n",
    "# Get paths to face images and emotion labels\n",
    "imagedir = []\n",
    "labeldir = []\n",
    "root, dirs, files = os.walk(\"./cohn-kanade-images2\",).__next__()\n",
    "for d in dirs:\n",
    "    imagedir.append(os.path.join(root,d))\n",
    "    labeldir.append(os.path.join('./Emotion',d))\n",
    "\n",
    "# initialize variables\n",
    "dataset = []\n",
    "NLA = []\n",
    "ELA = []\n",
    "MLA = []\n",
    "\n",
    "# look over all directories of individuals S001 through S999\n",
    "for d in dirs:\n",
    "    for dd in os.listdir(os.path.join('./cohn-kanade-images2',d)):\n",
    "        # skip over .DS_Store files\n",
    "        if 'Store' not in dd:\n",
    "            curdir = os.path.join('./cohn-kanade-images2',d,dd)\n",
    "            files = os.listdir(curdir)\n",
    "            baseimage = os.path.join(curdir,files[0])\n",
    "            midimage = os.path.join(curdir,files[int(len(files)/2)])\n",
    "            faceimage = os.path.join(curdir,files[-1])\n",
    "\n",
    "            # get directories of multiple expressions of same person\n",
    "            labeldir = os.path.join('./Emotion',d,dd)\n",
    "            if os.path.isdir(labeldir):\n",
    "                labels = os.listdir(labeldir)\n",
    "                # read label file if it exists\n",
    "                if len(labels)>0:\n",
    "                    f=open(os.path.join(labeldir,labels[0]),\"r\")\n",
    "                    for line in f:\n",
    "                        targetlabel = float(line)\n",
    "                    f.close()\n",
    "                    \n",
    "                    # append label to dataset\n",
    "                    dataset.append([baseimage, faceimage, midimage, targetlabel])\n",
    "\n",
    "\"\"\"68-POINT LANDMARK DETECTION MODEL\"\"\"\n",
    "# Get array of Euclidean differences between neutral and expression 68-landmark arrays\n",
    "tempLDA = []\n",
    "for imgIndex in range(len(dataset)):\n",
    "    nImgName = dataset[imgIndex][0]\n",
    "    eImgName = dataset[imgIndex][1]\n",
    "    mImgName = dataset[imgIndex][2]\n",
    "    nLA = get68LMA(nImgName)\n",
    "    eLA = get68LMA(eImgName)\n",
    "    mLA = get68LMA(mImgName)\n",
    "    NLA.append(nLA)\n",
    "    ELA.append(eLA)\n",
    "    MLA.append(mLA)\n",
    "#    tempLDA.append(getEucDist(nLA, eLA))\n",
    "#LDA = np.asmatrix(tempLDA)\n",
    "\n",
    "# Create target value array for expression images\n",
    "tempTargetValues = []\n",
    "for imgSet in range(len(dataset)):\n",
    "    if dataset[imgSet][3] == 0:\n",
    "        tempTargetValues.append(0.)\n",
    "        tempTargetValues.append(0.)    \n",
    "    else:\n",
    "        tempTargetValues.append(dataset[imgSet][3])\n",
    "        tempTargetValues.append(dataset[imgSet][3])    \n",
    "targetValues = np.asarray(tempTargetValues)\n",
    "\n",
    "# # Feed LDA and targetValues into SVM, use cross validation, generate confusion matrix\n",
    "# x_train, x_test, y_train, y_test = cross_validation.train_test_split(LDA, targetValues, test_size=0.4, random_state=2)\n",
    "# normCLF = svm.SVC(kernel='linear').fit(x_train, y_train)\n",
    "# predictions = normCLF.predict(x_test)\n",
    "# cmatrix = confusion_matrix(y_test, predictions)\n",
    "\n",
    "# # Format CMatrix to show percentages, rounded to 1 decimal point\n",
    "# np.set_printoptions(precision=1)\n",
    "# rowSums = np.sum(cmatrix, axis=1)\n",
    "# pCMatrix = np.divide(cmatrix*100., rowSums, dtype=float)\n",
    "# print pCMatrix\n",
    "\n",
    "\n",
    "\"\"\"PHYSIOLOGICALLY BASED LANDMARK DETECTION MODEL\"\"\"\n",
    "# Get array of corresponding differences between 13 distances of base images and expression images\n",
    "tempPDA = []\n",
    "for i in range(len(NLA)):\n",
    "    tempPDA.append(getPDA(NLA[i])-getPDA(ELA[i]))\n",
    "    tempPDA.append(getPDA(NLA[i])-getPDA(MLA[i]))\n",
    "PDA = np.asmatrix(tempPDA)\n",
    "#np.savetxt('PDAfile', PDA) #Save PDA as human-readable format file\n",
    "\n",
    "# Feed PDA and targetValues into SVM, use cross validation, generate confusion matrix\n",
    "x_train2, x_test2, y_train2, y_test2 = cross_validation.train_test_split(PDA, targetValues, test_size=0.4, random_state=3)\n",
    "physCLF = svm.SVC(kernel='rbf').fit(x_train2, y_train2)\n",
    "predictions2 = physCLF.predict(x_test2)\n",
    "cMatrix2 = confusion_matrix(y_test2, predictions2)\n",
    "\n",
    "# Format CMatrix2 to show percentages, already rounded to 1 decimal point\n",
    "# rowSums2 = np.sum(cMatrix2, axis=1)\n",
    "# pCMatrix2 = np.divide(cMatrix2*100., rowSums2, dtype=float)\n",
    "print(cMatrix2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./cohn-kanade-images2/S010/002/S010_002_00000001.png',\n",
       " './cohn-kanade-images2/S010/002/S010_002_00000014.png',\n",
       " './cohn-kanade-images2/S010/002/S010_002_00000008.png',\n",
       " 7.0]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  29.15475947,  168.19036833,  141.03191128,   30.59411708,\n",
       "         64.89992296,   89.28605714,  141.15594213,  120.2081528 ,\n",
       "         31.06444913,   68.26419266,  151.60804728,   40.16217126,\n",
       "         29.73213749])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "getPDA(NLA[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  28.7923601 ,  168.        ,  141.35416513,   26.83281573,\n",
       "         61.84658438,   88.32326987,  132.69890731,  118.37651794,\n",
       "         33.13608305,   71.55417528,  145.49914089,   37.64306045,\n",
       "         26.62705391])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "getPDA(ELA[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  29.15475947,  168.        ,  141.42842713,   27.31300057,\n",
       "         62.64982043,   88.88756943,  134.89254983,  118.53269591,\n",
       "         32.01562119,   70.23531875,  147.24808997,   37.64306045,\n",
       "         29.15475947])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "getPDA(MLA[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 100.            0.            0.            0.            0.            0.\n",
      "     0.            0.            0.            0.            0.            0.\n",
      "     0.        ]\n",
      " [   0.            9.09090909    5.            0.            3.7037037\n",
      "     0.           48.27586207   44.44444444    0.            0.            0.\n",
      "     0.            0.        ]\n",
      " [   0.            0.           25.            0.            3.7037037\n",
      "     0.           44.82758621    0.            4.76190476    0.            0.\n",
      "     0.            0.        ]\n",
      " [   0.            0.            0.            0.            0.            0.\n",
      "    44.82758621    0.            0.            0.            0.            0.\n",
      "     0.        ]\n",
      " [   0.            0.            5.            0.           44.44444444\n",
      "     0.           37.93103448    0.            0.            0.\n",
      "    10.71428571    0.            0.        ]\n",
      " [   0.            0.            5.            0.            0.            0.\n",
      "    31.03448276    0.            0.            0.            0.\n",
      "    16.66666667    0.        ]\n",
      " [   0.            0.            5.            0.            0.            0.\n",
      "    86.20689655    0.            0.            0.            0.            0.\n",
      "     8.57142857]\n",
      " [   0.            0.            5.            0.            0.            0.\n",
      "    27.5862069     0.            0.            0.            0.            0.\n",
      "     0.        ]\n",
      " [   0.            0.           10.            0.            0.            0.\n",
      "    62.06896552   11.11111111    0.            0.            0.            0.\n",
      "     0.        ]\n",
      " [   0.            0.            0.            0.            0.            0.\n",
      "    41.37931034    0.            0.            0.            0.            0.\n",
      "     0.        ]\n",
      " [   0.            0.            0.            0.           18.51851852\n",
      "     0.           44.82758621    0.            0.            0.\n",
      "    35.71428571    0.            0.        ]\n",
      " [   0.            0.            0.            0.            0.\n",
      "     9.09090909   17.24137931    0.            0.            0.            0.\n",
      "     0.            0.        ]\n",
      " [   0.            0.            0.            0.            0.            0.\n",
      "   106.89655172    0.            0.            0.            0.            0.\n",
      "    11.42857143]]\n"
     ]
    }
   ],
   "source": [
    "# RBF\n",
    "# SVM CLASSIFIERS FOR STRONG EMOTION EXPRESSIONS\n",
    "# Create 327x3 matrix, for each image set contains neutral image name, expression image name, emotion label\n",
    "# Generate array of normalized Euclidean physiological distances\n",
    "# Generate target values array\n",
    "# Feed to SVM, evaulate with Cross Validation\n",
    "\n",
    "import os\n",
    "\n",
    "# Initialize variables\n",
    "#imagedir = []\n",
    "labeldir = []\n",
    "dataset = []\n",
    "\n",
    "# Get paths to face images and emotion labels\n",
    "root, dirs, files = os.walk(\"./cohn-kanade-images2\").__next__()\n",
    "for d in dirs:\n",
    "    #imagedir.append(os.path.join(root,d))\n",
    "    labeldir.append(os.path.join('./Emotion',d))\n",
    "           \n",
    "# Look over all directories of individuals S001 through S999\n",
    "for person in dirs:\n",
    "    for dd in os.listdir(os.path.join('./cohn-kanade-images2',person)):\n",
    "        # skip over .DS_Store files\n",
    "        if 'Store' not in dd:\n",
    "            curdir = os.path.join('./cohn-kanade-images2',person,dd)\n",
    "            files = os.listdir(curdir)\n",
    "            baseimage = os.path.join(curdir,files[0])\n",
    "            midimage= os.path.join(curdir,files[int(len(files)/2)])\n",
    "            faceimage = os.path.join(curdir,files[-1])\n",
    "            # get directories of multiple expressions of same person\n",
    "            labeldir = os.path.join('./Emotion',person,dd)\n",
    "            if os.path.isdir(labeldir):\n",
    "                labels = os.listdir(labeldir)\n",
    "                # read label file if it exists\n",
    "                if len(labels)>0:\n",
    "                    f=open(os.path.join(labeldir,labels[0]),\"r\")\n",
    "                    for line in f:\n",
    "                        targetlabel = float(line)\n",
    "                    f.close()                    \n",
    "                    # append label to dataset\n",
    "                    if int(targetlabel)!=2:\n",
    "                        dataset.append([baseimage, midimage, faceimage, targetlabel])\n",
    "\n",
    "\"\"\"GET DATA TO TRAIN MODEL\"\"\"\n",
    "#1: Get array of corresponding differences between 13 distances of base images and strong/moderate expression images\n",
    "tempPDA = []\n",
    "for imgIndex in range(len(dataset)):\n",
    "    tempPDA.append(getPhysDA(get68LMA(dataset[imgIndex][0]))-getPhysDA(get68LMA(dataset[imgIndex][2])))\n",
    "    tempPDA.append(getPhysDA(get68LMA(dataset[imgIndex][0]))-getPhysDA(get68LMA(dataset[imgIndex][1])))\n",
    "    tempPDA.append([0]*13)\n",
    "physDA = np.asmatrix(tempPDA)\n",
    "np.savetxt('PDAfile', physDA) #Save PDA as human-readable format file\n",
    "\n",
    "#2: Create target value array for expression images\n",
    "tempTargVals = []\n",
    "for imgSet in range(len(dataset)):\n",
    "    if dataset[imgSet][3]==0:                      #if expression sequence is neutral\n",
    "        tempTargVals.append(0.)\n",
    "        tempTargVals.append(0.)\n",
    "    else:                                         #if expression sequence has emotion\n",
    "        tempTargVals.append(dataset[imgSet][3]+20.)\n",
    "        tempTargVals.append(dataset[imgSet][3]+10.)\n",
    "    tempTargVals.append(0.)\n",
    "targetValues = np.asarray(tempTargVals)\n",
    "\n",
    "\"\"\"FEED DATA TO TRAIN MODEL\"\"\"\n",
    "# Feed PDA and targetValues into SVM, use cross validation, generate confusion matrix\n",
    "x_train, x_test, y_train, y_test = cross_validation.train_test_split(physDA, targetValues, test_size=0.4, random_state=3)\n",
    "physCLF = svm.SVC(kernel='rbf').fit(x_train, y_train)\n",
    "predictions = physCLF.predict(x_test)\n",
    "cMatrix = confusion_matrix(y_test, predictions)\n",
    "\n",
    "\"\"\"EVAULUATE USING READABLE CONFUSION MATRIX\"\"\"\n",
    "# Format CMatrix to show percentages, already rounded to 1 decimal point\n",
    "rowSums = np.sum(cMatrix, axis=1)\n",
    "pCMatrix = np.divide(cMatrix*100., rowSums, dtype=float)\n",
    "print(pCMatrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['physModelRBF.pkl',\n",
       " 'physModelRBF.pkl_01.npy',\n",
       " 'physModelRBF.pkl_02.npy',\n",
       " 'physModelRBF.pkl_03.npy',\n",
       " 'physModelRBF.pkl_04.npy',\n",
       " 'physModelRBF.pkl_05.npy',\n",
       " 'physModelRBF.pkl_06.npy',\n",
       " 'physModelRBF.pkl_07.npy',\n",
       " 'physModelRBF.pkl_08.npy',\n",
       " 'physModelRBF.pkl_09.npy',\n",
       " 'physModelRBF.pkl_10.npy',\n",
       " 'physModelRBF.pkl_11.npy']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.externals import joblib\n",
    "joblib.dump(physCLF, 'physModelRBF.pkl') \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Capture video from robot camera\n",
    "# Show video in color\n",
    "# Display box around face\n",
    "# Does NOT display facial landmarks for each face\n",
    "# SLOW Face Tracking\n",
    "\n",
    "import cv2\n",
    "import dlib\n",
    "import nxt.locator\n",
    "from nxt.motor import *\n",
    "\n",
    "cap = cv2.VideoCapture(1) # capture video from robot camera\n",
    "cap.set(3,640)\n",
    "cap.set(4,480)\n",
    "b = nxt.locator.find_one_brick() # connect to NXT brick\n",
    "\n",
    "# # Initialize variables for dLib\n",
    "# predictor_path = \"/Users/kaili/Dropbox/HSSF1516/shape_predictor_68_face_landmarks.dat\"\n",
    "# detector = dlib.get_frontal_face_detector()\n",
    "# predictor = dlib.shape_predictor(predictor_path)\n",
    "\n",
    "images=[]\n",
    "\n",
    "while(True):\n",
    "    # Capture frame-by-frame\n",
    "    ret, frame = cap.read()\n",
    "    \n",
    "    # Detect faces and initialize variables for next step\n",
    "    dets = detector(frame, 1)\n",
    "    \n",
    "    # Draw a rectangle around the faces\n",
    "    for rect in dets:\n",
    "        cv2.rectangle(frame, (rect.left(), rect.top()), (rect.right(), rect.bottom()), (0, 255, 0), 2)\n",
    "        cv2.rectangle(frame, (0,0), (1,1), (0, 255, 0), 2)\n",
    "        facecenter=rect.center()\n",
    "        if facecenter.y>280:\n",
    "            Motor(b, PORT_B).turn(10, 10)\n",
    "        elif facecenter.y<200:\n",
    "            Motor(b, PORT_B).turn(-10, 10)\n",
    "        if facecenter.x>360:\n",
    "            Motor(b, PORT_A).turn(10, 10)\n",
    "        elif facecenter.x<280:\n",
    "            Motor(b, PORT_A).turn(-10, 10)\n",
    "        break\n",
    "\n",
    "    # Display frame, press 'q' to exit video window\n",
    "#     cv2.imshow('frame',frame)\n",
    "#      if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "#         break\n",
    "\n",
    "# When everything done, release the capture\n",
    "cap.release()\n",
    "# cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Method to get 68 landmarks for IMAGE passed to the function\n",
    "def getI68LMA(img):\n",
    "    predictor_path = \"/Users/kaili/Dropbox/HSSF1516/shape_predictor_68_face_landmarks.dat\"\n",
    "    detector = dlib.get_frontal_face_detector()\n",
    "    predictor = dlib.shape_predictor(predictor_path)\n",
    "    LMA = []\n",
    "    \n",
    "    # Detect face(s)\n",
    "    dets = detector(img, 1)\n",
    "\n",
    "    # Get list of coordinates of 68 facial landmarks for each face\n",
    "    if len(dets)>0:\n",
    "        shape = predictor(img, dets[0])\n",
    "        for index in range(0, 68):\n",
    "            point = shape.part(index)\n",
    "            tuplepoint = (point.x, point.y)\n",
    "            LMA.append(tuplepoint)\n",
    "        return LMA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 17.]\n",
      "Neutral\n",
      "[ 16.]\n",
      "Neutral\n",
      "[ 11.]\n",
      "Neutral\n",
      "[ 0.]\n",
      "Neutral\n",
      "[ 16.]\n",
      "Neutral\n",
      "[ 0.]\n",
      "Neutral\n",
      "[ 0.]\n",
      "Neutral\n",
      "[ 0.]\n",
      "Neutral\n",
      "[ 11.]\n",
      "Neutral\n",
      "[ 16.]\n",
      "Neutral\n",
      "[ 0.]\n",
      "Neutral\n",
      "[ 0.]\n",
      "Neutral\n",
      "[ 0.]\n",
      "Neutral\n",
      "[ 16.]\n",
      "Neutral\n",
      "[ 0.]\n",
      "Neutral\n",
      "[ 16.]\n",
      "Neutral\n",
      "[ 17.]\n",
      "Neutral\n",
      "[ 14.]\n",
      "Neutral\n",
      "[ 13.]\n",
      "Neutral\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-8b9b7cd933cc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0;31m# Get physiological feature vector for current frame\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m     \u001b[0mimages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgetPhysDA\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgetI68LMA\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-0ac771d50fa0>\u001b[0m in \u001b[0;36mgetI68LMA\u001b[0;34m(img)\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mpredictor_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"/Users/kaili/Dropbox/HSSF1516/shape_predictor_68_face_landmarks.dat\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mdetector\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_frontal_face_detector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mpredictor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape_predictor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictor_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mLMA\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Capture video\n",
    "# Classify emotion\n",
    "# Output the emotion\n",
    "\n",
    "import cv2\n",
    "import dlib\n",
    "import numpy as np\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "cap = cv2.VideoCapture(0) # capture video from robot camera\n",
    "cap.set(3,640)\n",
    "cap.set(4,480)\n",
    "loaded = joblib.load('physModel.pkl')\n",
    "detector = dlib.get_frontal_face_detector()\n",
    "\n",
    "images=[]\n",
    "\n",
    "while(True):\n",
    "    # Capture frame-by-frame\n",
    "    ret, frame = cap.read()\n",
    "    dets = detector(frame, 1)\n",
    "    \n",
    "    for rect in dets:\n",
    "        cv2.rectangle(frame, (rect.left(), rect.top()), (rect.right(), rect.bottom()), (0, 255, 0), 2)\n",
    "    cv2.imshow('frame',frame)\n",
    "\n",
    "    # Get physiological feature vector for current frame\n",
    "    images.append(getPhysDA(getI68LMA(frame)))\n",
    "    \n",
    "    if len(images)>2:\n",
    "        images.pop(0)\n",
    "    # Classify expression in image\n",
    "    if len(images)>1:\n",
    "        prob=loaded.predict(np.asarray(images[0]-images[1])) # Get array of probabilities\n",
    "        index = np.argmax(prob)# Get position of highest probability\n",
    "        print(prob)\n",
    "        \n",
    "        # Print the emotion corresponding to the position\n",
    "        if index==0:\n",
    "            print(\"Neutral\")\n",
    "        elif index==1:\n",
    "            print(\"Moderate Anger\")\n",
    "        elif index==2:\n",
    "            print(\"Moderate Disgust\")\n",
    "        elif index==3:\n",
    "            print(\"Moderate Fear\")\n",
    "        elif index==4:\n",
    "            print(\"Moderate Happiness\")\n",
    "        elif index==5:\n",
    "            print(\"Moderate Sadness\")\n",
    "        elif index==6:\n",
    "            print(\"Moderate Surprise\")\n",
    "        elif index==7:\n",
    "            print(\"Strong Anger\")\n",
    "        elif index==8:\n",
    "            print(\"Strong Disgust\")\n",
    "        elif index==9:\n",
    "            print(\"Strong Fear\")\n",
    "        elif index==10:\n",
    "            print(\"Strong Happiness\")\n",
    "        elif index==11:\n",
    "            print(\"Strong Sadness\")\n",
    "        elif index==12:\n",
    "            print(\"Strong Surprise\")\n",
    "            \n",
    "    # Break out of while loop if 'q' key pressed\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break      \n",
    "        \n",
    "# Release video capture\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cap.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# SET UP METHODS FOR SVM\n",
    "\n",
    "import numpy as np\n",
    "import cv2\n",
    "import dlib\n",
    "import os\n",
    "import math\n",
    "from sklearn import svm\n",
    "from sklearn import cross_validation\n",
    "from sklearn.metrics import confusion_matrix    \n",
    "from sklearn.externals import joblib\n",
    "\n",
    "# Function that returns the distance between two given points\n",
    "def ptDist(pt1, pt2):\n",
    "    x1 = pt1[0]\n",
    "    x2 = pt2[0]\n",
    "    y1 = pt1[1]\n",
    "    y2 = pt2[1]\n",
    "    return math.sqrt((x1-x2)*(x1-x2) + (y1-y2)*(y1-y2))\n",
    "\n",
    "# Function that returns array of 68 landmarks for given image name\n",
    "def get68LMA(imgName):\n",
    "    predictor_path = \"/Users/kaili/Dropbox/HSSF1516/shape_predictor_68_face_landmarks.dat\"\n",
    "    detector = dlib.get_frontal_face_detector()\n",
    "    predictor = dlib.shape_predictor(predictor_path)\n",
    "    LMA = []\n",
    "    image = cv2.imread(imgName, 1)\n",
    "    \n",
    "    # Detect face(s)\n",
    "    dets = detector(image, 1)\n",
    "\n",
    "    # Get list of coordinates of 68 facial landmarks for each face\n",
    "    shape = predictor(image, dets[0])\n",
    "    for index in range(0, 68):\n",
    "        point = shape.part(index)\n",
    "        tuplepoint = (point.x, point.y)\n",
    "        LMA.append(tuplepoint)\n",
    "    return LMA\n",
    "\n",
    "# Function that eturns normalized array of Euclidean distances given two arrays of coordinates\n",
    "def getEucDist(nLA, eLA):\n",
    "    LDA = []\n",
    "    sum = 0\n",
    "    # Get array of distances\n",
    "    for pt in range(68):\n",
    "        dif = ptDist(eLA[pt], nLA[pt])     \n",
    "        LDA.append(dif)\n",
    "        sum += dif\n",
    "    avg = sum/68\n",
    "    # Normalize array\n",
    "    for i in range(68):\n",
    "        LDA[i] -= avg      \n",
    "    return LDA\n",
    "\n",
    "# Function that returns 13-point numpy vector of distances between selected physiological pts, given the 68-point vector\n",
    "def getPhysDA(iLA):\n",
    "    PDA = []\n",
    "    PDA.append(ptDist(iLA[20], iLA[21])) #distance 0: distance between brows\n",
    "    PDA.append(ptDist(iLA[20], iLA[26])) #distance 1: inner brow to nose- left\n",
    "    PDA.append(ptDist(iLA[21], iLA[26])) #distance 2: inner brow to nose- right\n",
    "    PDA.append(ptDist(iLA[20], iLA[38])) #distance 3: inner brow to inner eye corner- left\n",
    "    PDA.append(ptDist(iLA[21], iLA[41])) #distance 4: inner brow to inner eye corner- right\n",
    "    PDA.append(ptDist(iLA[35], iLA[47])) #distance 5: outer eye corner to outer mouth corner- left\n",
    "    PDA.append(ptDist(iLA[44], iLA[53])) #distance 6: outer eye corner to outer mouth corner- right\n",
    "    PDA.append(ptDist(iLA[47], iLA[53])) #distance 7: distance between mouth corners\n",
    "    PDA.append(ptDist(iLA[61], iLA[65])) #distance 8: height of mouth\n",
    "    PDA.append(ptDist(iLA[35], iLA[49])) #distance 9: raising of upper lip\n",
    "    PDA.append(ptDist(iLA[44], iLA[51])) #distance 10: raising of upper lip\n",
    "    PDA.append(ptDist(iLA[18], iLA[36])) #distance 11: brow arch- left\n",
    "    PDA.append(ptDist(iLA[23], iLA[43])) #distance 12: brow arch- right\n",
    "    return np.asarray(PDA)\n",
    "# Method to get 68 landmarks for IMAGE passed to the function\n",
    "def getI68LMA(img):\n",
    "    predictor_path = \"/Users/kaili/Dropbox/HSSF1516/shape_predictor_68_face_landmarks.dat\"\n",
    "    detector = dlib.get_frontal_face_detector()\n",
    "    predictor = dlib.shape_predictor(predictor_path)\n",
    "    LMA = []\n",
    "    \n",
    "    # Detect face(s)\n",
    "    dets = detector(img, 1)\n",
    "\n",
    "    # Get list of coordinates of 68 facial landmarks for each face\n",
    "    if len(dets)>0:\n",
    "        shape = predictor(img, dets[0])\n",
    "        for index in range(0, 68):\n",
    "            point = shape.part(index)\n",
    "            tuplepoint = (point.x, point.y)\n",
    "            LMA.append(tuplepoint)\n",
    "        return LMA\n",
    "    \n",
    "    \n",
    "\n",
    "cap = cv2.VideoCapture(0) # capture video from robot camera\n",
    "cap.set(3,640)\n",
    "cap.set(4,480)\n",
    "model = joblib.load('physModel.pkl')\n",
    "detector = dlib.get_frontal_face_detector()\n",
    "\n",
    "images=[]\n",
    "\n",
    "while(True):\n",
    "    # Capture frame-by-frame\n",
    "    ret, frame = cap.read()\n",
    "    dets = detector(frame, 1)\n",
    "    \n",
    "    for rect in dets:\n",
    "        cv2.rectangle(frame, (rect.left(), rect.top()), (rect.right(), rect.bottom()), (0, 255, 0), 2)\n",
    "    cv2.imshow('frame',frame)\n",
    "\n",
    "    # Get physiological feature vector for current frame\n",
    "    if len(dets)>0:\n",
    "        images.append(getPhysDA(getI68LMA(frame)))\n",
    "    \n",
    "        if len(images)>2:\n",
    "            images.pop(0)\n",
    "        # Classify expression in image\n",
    "        if len(images)>1:\n",
    "            index=model.predict(np.array(images[0]-images[1]).reshape(1,-1)) # Get array of probabilities\n",
    "\n",
    "            # Print the emotion corresponding to the position\n",
    "            if index==0:\n",
    "                print(\"Neutral\")\n",
    "            elif index==11:\n",
    "                print(\"Moderate Anger\")\n",
    "            elif index==13:\n",
    "                print(\"Moderate Disgust\")\n",
    "            elif index==14:\n",
    "                print(\"Moderate Fear\")\n",
    "            elif index==15:\n",
    "                print(\"Moderate Happiness\")\n",
    "            elif index==16:\n",
    "                print(\"Moderate Sadness\")\n",
    "            elif index==17:\n",
    "                print(\"Moderate Surprise\")\n",
    "            elif index==21:\n",
    "                print(\"Strong Anger\")\n",
    "            elif index==23:\n",
    "                print(\"Strong Disgust\")\n",
    "            elif index==24:\n",
    "                print(\"Strong Fear\")\n",
    "            elif index==25:\n",
    "                print(\"Strong Happiness\")\n",
    "            elif index==26:\n",
    "                print(\"Strong Sadness\")\n",
    "            elif index==27:\n",
    "                print(\"Strong Surprise\")\n",
    "            \n",
    "    # Break out of while loop if 'q' key pressed\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break      \n",
    "        \n",
    "# Release video capture\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[138   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0  13   9   0   0   0   0   0]\n",
      " [  0   0   2   0   0   0  11   0   7   0   0   0   0]\n",
      " [  0   0   0   0   0   0  13   0   0   0   0   0   0]\n",
      " [  0   0   0   0   7   0   5   0   0   0  15   0   0]\n",
      " [  0   0   0   0   0   0  10   0   0   0   0   1   0]\n",
      " [  0   0   0   0   0   0  24   0   0   0   0   0   5]\n",
      " [  0   0   0   0   0   0   9   0   0   0   0   0   0]\n",
      " [  0   0  12   0   0   0   8   1   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0  12   0   0   0   0   0   0]\n",
      " [  0   0   0   0  17   0   8   0   0   0   3   0   0]\n",
      " [  0   0   0   0   0   0   6   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0  35   0   0   0   0   0   0]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['RBF34.pkl',\n",
       " 'RBF34.pkl_01.npy',\n",
       " 'RBF34.pkl_02.npy',\n",
       " 'RBF34.pkl_03.npy',\n",
       " 'RBF34.pkl_04.npy',\n",
       " 'RBF34.pkl_05.npy',\n",
       " 'RBF34.pkl_06.npy',\n",
       " 'RBF34.pkl_07.npy',\n",
       " 'RBF34.pkl_08.npy',\n",
       " 'RBF34.pkl_09.npy',\n",
       " 'RBF34.pkl_10.npy',\n",
       " 'RBF34.pkl_11.npy']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# RBF 3/4 MOD IMG\n",
    "# SVM CLASSIFIERS FOR STRONG EMOTION EXPRESSIONS\n",
    "# Create 327x3 matrix, for each image set contains neutral image name, expression image name, emotion label\n",
    "# Generate array of normalized Euclidean physiological distances\n",
    "# Generate target values array\n",
    "# Feed to SVM, evaulate with Cross Validation\n",
    "\n",
    "import os\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "# Initialize variables\n",
    "#imagedir = []\n",
    "labeldir = []\n",
    "dataset = []\n",
    "\n",
    "# Get paths to face images and emotion labels\n",
    "root, dirs, files = os.walk(\"./cohn-kanade-images2\").__next__()\n",
    "for d in dirs:\n",
    "    #imagedir.append(os.path.join(root,d))\n",
    "    labeldir.append(os.path.join('./Emotion',d))\n",
    "           \n",
    "# Look over all directories of individuals S001 through S999\n",
    "for person in dirs:\n",
    "    for dd in os.listdir(os.path.join('./cohn-kanade-images2',person)):\n",
    "        # skip over .DS_Store files\n",
    "        if 'Store' not in dd:\n",
    "            curdir = os.path.join('./cohn-kanade-images2',person,dd)\n",
    "            files = os.listdir(curdir)\n",
    "            baseimage = os.path.join(curdir,files[0])\n",
    "            midimage= os.path.join(curdir,files[int(len(files)*3/4)])\n",
    "            faceimage = os.path.join(curdir,files[-1])\n",
    "            # get directories of multiple expressions of same person\n",
    "            labeldir = os.path.join('./Emotion',person,dd)\n",
    "            if os.path.isdir(labeldir):\n",
    "                labels = os.listdir(labeldir)\n",
    "                # read label file if it exists\n",
    "                if len(labels)>0:\n",
    "                    f=open(os.path.join(labeldir,labels[0]),\"r\")\n",
    "                    for line in f:\n",
    "                        targetlabel = float(line)\n",
    "                    f.close()                    \n",
    "                    # append label to dataset\n",
    "                    if int(targetlabel)!=2:\n",
    "                        dataset.append([baseimage, midimage, faceimage, targetlabel])\n",
    "\n",
    "\"\"\"GET DATA TO TRAIN MODEL\"\"\"\n",
    "#1: Get array of corresponding differences between 13 distances of base images and strong/moderate expression images\n",
    "tempPDA = []\n",
    "for imgIndex in range(len(dataset)):\n",
    "    tempPDA.append(getPhysDA(get68LMA(dataset[imgIndex][0]))-getPhysDA(get68LMA(dataset[imgIndex][2])))\n",
    "    tempPDA.append(getPhysDA(get68LMA(dataset[imgIndex][0]))-getPhysDA(get68LMA(dataset[imgIndex][1])))\n",
    "    tempPDA.append([0]*13)\n",
    "physDA = np.asmatrix(tempPDA)\n",
    "np.savetxt('PDAfile', physDA) #Save PDA as human-readable format file\n",
    "\n",
    "#2: Create target value array for expression images\n",
    "tempTargVals = []\n",
    "for imgSet in range(len(dataset)):\n",
    "    if dataset[imgSet][3]==0:                      #if expression sequence is neutral\n",
    "        tempTargVals.append(0.)\n",
    "        tempTargVals.append(0.)\n",
    "    else:                                         #if expression sequence has emotion\n",
    "        tempTargVals.append(dataset[imgSet][3]+20.)\n",
    "        tempTargVals.append(dataset[imgSet][3]+10.)\n",
    "    tempTargVals.append(0.)\n",
    "targetValues = np.asarray(tempTargVals)\n",
    "\n",
    "\"\"\"FEED DATA TO TRAIN MODEL\"\"\"\n",
    "# Feed PDA and targetValues into SVM, use cross validation, generate confusion matrix\n",
    "x_train, x_test, y_train, y_test = cross_validation.train_test_split(physDA, targetValues, test_size=0.4, random_state=3)\n",
    "physCLF = svm.SVC(kernel='rbf').fit(x_train, y_train)\n",
    "predictions = physCLF.predict(x_test)\n",
    "cMatrix = confusion_matrix(y_test, predictions)\n",
    "\n",
    "\"\"\"EVAULUATE USING READABLE CONFUSION MATRIX\"\"\"\n",
    "# Format CMatrix to show percentages, already rounded to 1 decimal point\n",
    "# rowSums = np.sum(cMatrix, axis=1)\n",
    "# pCMatrix = np.divide(cMatrix*100., rowSums, dtype=float)\n",
    "print(cMatrix)\n",
    "joblib.dump(physCLF, 'RBF34.pkl') \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[138   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0  13   9   0   0   0   0   0]\n",
      " [  0   0   2   0   0   0  11   0   7   0   0   0   0]\n",
      " [  0   0   0   0   0   0  13   0   0   0   0   0   0]\n",
      " [  0   0   0   0   7   0   5   0   0   0  15   0   0]\n",
      " [  0   0   0   0   0   0  10   0   0   0   0   1   0]\n",
      " [  0   0   0   0   0   0  24   0   0   0   0   0   5]\n",
      " [  0   0   0   0   0   0   9   0   0   0   0   0   0]\n",
      " [  0   0  12   0   0   0   8   1   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0  12   0   0   0   0   0   0]\n",
      " [  0   0   0   0  17   0   8   0   0   0   3   0   0]\n",
      " [  0   0   0   0   0   0   6   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0  35   0   0   0   0   0   0]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['linear34.pkl',\n",
       " 'linear34.pkl_01.npy',\n",
       " 'linear34.pkl_02.npy',\n",
       " 'linear34.pkl_03.npy',\n",
       " 'linear34.pkl_04.npy',\n",
       " 'linear34.pkl_05.npy',\n",
       " 'linear34.pkl_06.npy',\n",
       " 'linear34.pkl_07.npy',\n",
       " 'linear34.pkl_08.npy',\n",
       " 'linear34.pkl_09.npy',\n",
       " 'linear34.pkl_10.npy',\n",
       " 'linear34.pkl_11.npy']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# RBF 3/4 MOD IMG\n",
    "# SVM CLASSIFIERS FOR STRONG EMOTION EXPRESSIONS\n",
    "# Create 327x3 matrix, for each image set contains neutral image name, expression image name, emotion label\n",
    "# Generate array of normalized Euclidean physiological distances\n",
    "# Generate target values array\n",
    "# Feed to SVM, evaulate with Cross Validation\n",
    "\n",
    "import os\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "# Initialize variables\n",
    "#imagedir = []\n",
    "labeldir = []\n",
    "dataset = []\n",
    "\n",
    "# Get paths to face images and emotion labels\n",
    "root, dirs, files = os.walk(\"./cohn-kanade-images2\").__next__()\n",
    "for d in dirs:\n",
    "    #imagedir.append(os.path.join(root,d))\n",
    "    labeldir.append(os.path.join('./Emotion',d))\n",
    "           \n",
    "# Look over all directories of individuals S001 through S999\n",
    "for person in dirs:\n",
    "    for dd in os.listdir(os.path.join('./cohn-kanade-images2',person)):\n",
    "        # skip over .DS_Store files\n",
    "        if 'Store' not in dd:\n",
    "            curdir = os.path.join('./cohn-kanade-images2',person,dd)\n",
    "            files = os.listdir(curdir)\n",
    "            baseimage = os.path.join(curdir,files[0])\n",
    "            midimage= os.path.join(curdir,files[int(len(files)*3/4)])\n",
    "            faceimage = os.path.join(curdir,files[-1])\n",
    "            # get directories of multiple expressions of same person\n",
    "            labeldir = os.path.join('./Emotion',person,dd)\n",
    "            if os.path.isdir(labeldir):\n",
    "                labels = os.listdir(labeldir)\n",
    "                # read label file if it exists\n",
    "                if len(labels)>0:\n",
    "                    f=open(os.path.join(labeldir,labels[0]),\"r\")\n",
    "                    for line in f:\n",
    "                        targetlabel = float(line)\n",
    "                    f.close()                    \n",
    "                    # append label to dataset\n",
    "                    if int(targetlabel)!=2:\n",
    "                        dataset.append([baseimage, midimage, faceimage, targetlabel])\n",
    "\n",
    "\"\"\"GET DATA TO TRAIN MODEL\"\"\"\n",
    "#1: Get array of corresponding differences between 13 distances of base images and strong/moderate expression images\n",
    "tempPDA = []\n",
    "for imgIndex in range(len(dataset)):\n",
    "    tempPDA.append(getPhysDA(get68LMA(dataset[imgIndex][0]))-getPhysDA(get68LMA(dataset[imgIndex][2])))\n",
    "    tempPDA.append(getPhysDA(get68LMA(dataset[imgIndex][0]))-getPhysDA(get68LMA(dataset[imgIndex][1])))\n",
    "    tempPDA.append([0.]*13)\n",
    "physDA = np.asmatrix(tempPDA)\n",
    "np.savetxt('PDAfile', physDA) #Save PDA as human-readable format file\n",
    "\n",
    "#2: Create target value array for expression images\n",
    "tempTargVals = []\n",
    "for imgSet in range(len(dataset)):\n",
    "    if dataset[imgSet][3]==0:                      #if expression sequence is neutral\n",
    "        tempTargVals.append(0.)\n",
    "        tempTargVals.append(0.)\n",
    "    else:                                         #if expression sequence has emotion\n",
    "        tempTargVals.append(dataset[imgSet][3]+20.)\n",
    "        tempTargVals.append(dataset[imgSet][3]+10.)\n",
    "    tempTargVals.append(0.)\n",
    "targetValues = np.asarray(tempTargVals)\n",
    "\n",
    "\"\"\"FEED DATA TO TRAIN MODEL\"\"\"\n",
    "# Feed PDA and targetValues into SVM, use cross validation, generate confusion matrix\n",
    "x_train, x_test, y_train, y_test = cross_validation.train_test_split(physDA, targetValues, test_size=0.4, random_state=3)\n",
    "lin34CLF = svm.SVC(kernel='linear').fit(x_train, y_train)\n",
    "predictions = physCLF.predict(x_test)\n",
    "L34cMatrix = confusion_matrix(y_test, predictions)\n",
    "\n",
    "\"\"\"EVAULUATE USING READABLE CONFUSION MATRIX\"\"\"\n",
    "# Format CMatrix to show percentages, already rounded to 1 decimal point\n",
    "# rowSums = np.sum(cMatrix, axis=1)\n",
    "# pCMatrix = np.divide(cMatrix*100., rowSums, dtype=float)\n",
    "print(L34cMatrix)\n",
    "joblib.dump(lin34CLF, 'linear34.pkl') \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 9  2  0  1  7  3  6  1  0  0  0  0]\n",
      " [ 0 17  0  0  2  0  1  5  0  0  0  0]\n",
      " [ 1  0  3  1  1  2  0  0  2  1  1  0]\n",
      " [ 0  1  3 19  0  1  0  0  1 10  0  0]\n",
      " [ 0  0  0  0  1  1  0  0  1  0  3  0]\n",
      " [ 0  0  3  0  0 10  0  0  2  0  1  5]\n",
      " [11  2  0  0  0  0  3  3  0  0  0  0]\n",
      " [ 1  6  0  1  1  0  1  7  0  1  0  0]\n",
      " [ 0  0  4  0  0  1  0  0  2  0  1  0]\n",
      " [ 0  0  0 16  0  0  0  0  1 20  0  0]\n",
      " [ 0  0  1  0  4  0  0  0  1  0  2  0]\n",
      " [ 0  0  2  1  0  8  0  0  2  0  0 17]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['poly13.pkl',\n",
       " 'poly13.pkl_01.npy',\n",
       " 'poly13.pkl_02.npy',\n",
       " 'poly13.pkl_03.npy',\n",
       " 'poly13.pkl_04.npy',\n",
       " 'poly13.pkl_05.npy',\n",
       " 'poly13.pkl_06.npy',\n",
       " 'poly13.pkl_07.npy',\n",
       " 'poly13.pkl_08.npy',\n",
       " 'poly13.pkl_09.npy',\n",
       " 'poly13.pkl_10.npy',\n",
       " 'poly13.pkl_11.npy']"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# RBF 3/4 MOD IMG\n",
    "# SVM CLASSIFIERS FOR STRONG EMOTION EXPRESSIONS\n",
    "# Create 327x3 matrix, for each image set contains neutral image name, expression image name, emotion label\n",
    "# Generate array of normalized Euclidean physiological distances\n",
    "# Generate target values array\n",
    "# Feed to SVM, evaulate with Cross Validation\n",
    "\n",
    "import os\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "# Initialize variables\n",
    "#imagedir = []\n",
    "labeldir = []\n",
    "dataset = []\n",
    "\n",
    "# Get paths to face images and emotion labels\n",
    "root, dirs, files = os.walk(\"./cohn-kanade-images2\").__next__()\n",
    "for d in dirs:\n",
    "    #imagedir.append(os.path.join(root,d))\n",
    "    labeldir.append(os.path.join('./Emotion',d))\n",
    "           \n",
    "# Look over all directories of individuals S001 through S999\n",
    "for person in dirs:\n",
    "    for dd in os.listdir(os.path.join('./cohn-kanade-images2',person)):\n",
    "        # skip over .DS_Store files\n",
    "        if 'Store' not in dd:\n",
    "            curdir = os.path.join('./cohn-kanade-images2',person,dd)\n",
    "            files = os.listdir(curdir)\n",
    "            baseimage = os.path.join(curdir,files[0])\n",
    "            midimage= os.path.join(curdir,files[int(len(files)/2.)])\n",
    "            faceimage = os.path.join(curdir,files[-1])\n",
    "            # get directories of multiple expressions of same person\n",
    "            labeldir = os.path.join('./Emotion',person,dd)\n",
    "            if os.path.isdir(labeldir):\n",
    "                labels = os.listdir(labeldir)\n",
    "                # read label file if it exists\n",
    "                if len(labels)>0:\n",
    "                    f=open(os.path.join(labeldir,labels[0]),\"r\")\n",
    "                    for line in f:\n",
    "                        targetlabel = float(line)\n",
    "                    f.close()                    \n",
    "                    # append label to dataset\n",
    "                    if int(targetlabel)!=2:\n",
    "                        dataset.append([baseimage, midimage, faceimage, targetlabel])\n",
    "\n",
    "\"\"\"GET DATA TO TRAIN MODEL\"\"\"\n",
    "#1: Get array of corresponding differences between 13 distances of base images and strong/moderate expression images\n",
    "tempPDA = []\n",
    "for imgIndex in range(len(dataset)):\n",
    "    tempPDA.append(getPhysDA(get68LMA(dataset[imgIndex][0]))-getPhysDA(get68LMA(dataset[imgIndex][2])))\n",
    "    tempPDA.append(getPhysDA(get68LMA(dataset[imgIndex][0]))-getPhysDA(get68LMA(dataset[imgIndex][1])))\n",
    "physDA = np.asmatrix(tempPDA)\n",
    "np.savetxt('PDAfile', physDA) #Save PDA as human-readable format file\n",
    "\n",
    "#2: Create target value array for expression images\n",
    "tempTargVals = []\n",
    "for imgSet in range(len(dataset)):\n",
    "    tempTargVals.append(dataset[imgSet][3]+10.)\n",
    "    tempTargVals.append(dataset[imgSet][3])\n",
    "targetValues = np.asarray(tempTargVals)\n",
    "\n",
    "\"\"\"FEED DATA TO TRAIN MODEL\"\"\"\n",
    "# Feed PDA and targetValues into SVM, use cross validation, generate confusion matrix\n",
    "x_train, x_test, y_train, y_test = cross_validation.train_test_split(physDA, targetValues, test_size=0.4, random_state=3)\n",
    "polyCLF = svm.SVC(kernel='poly',probability=True).fit(x_train, y_train)\n",
    "predictions = polyCLF.predict(x_test)\n",
    "polycMatrix = confusion_matrix(y_test, predictions)\n",
    "\n",
    "\"\"\"EVAULUATE USING READABLE CONFUSION MATRIX\"\"\"\n",
    "# Format CMatrix to show percentages, already rounded to 1 decimal point\n",
    "# rowSums = np.sum(cMatrix, axis=1)\n",
    "# pCMatrix = np.divide(cMatrix*100., rowSums, dtype=float)\n",
    "print(polycMatrix)\n",
    "joblib.dump(polyCLF, 'poly13.pkl') \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
