{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "print cv2.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Capture Video on Camera\n",
    "# Shows video in gray scale in a new window\n",
    "# Adapted from: http://docs.opencv.org/3.0-beta/doc/py_tutorials/py_gui/py_video_display/py_video_display.html\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "while(True):\n",
    "    # Capture frame-by-frame\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    # Our operations on the frame come here\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Display the resulting frame\n",
    "    cv2.imshow('frame',gray)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# When everything done, release the capture\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Capture Video on Camera\n",
    "# Shows video in gray scale in a new window\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "cap = cv2.VideoCapture()\n",
    "\n",
    "while(True):\n",
    "    # Capture frame-by-frame\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    # Our operations on the frame come here\n",
    "    #gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Display the resulting frame\n",
    "    cv2.imshow('frame', frame)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# When everything done, release the capture\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Calculates Frames Per Second of Video/Camera\n",
    "# Adapted from: http://www.learnopencv.com/how-to-find-frame-rate-or-frames-per-second-fps-in-opencv-python-cpp/\n",
    "import cv2\n",
    "import time\n",
    " \n",
    "# Start default camera\n",
    "video = cv2.VideoCapture(0);\n",
    "\n",
    "# Find OpenCV version\n",
    "(major_ver, minor_ver, subminor_ver) = (cv2.__version__).split('.')\n",
    "\n",
    "# With webcam get(CV_CAP_PROP_FPS) does not work.\n",
    "# Let's see for ourselves.\n",
    "\n",
    "if int(major_ver)  < 3 :\n",
    "    fps = video.get(cv2.cv.CV_CAP_PROP_FPS)\n",
    "    print \"Frames per second using video.get(cv2.cv.CV_CAP_PROP_FPS): {0}\".format(fps)\n",
    "else :\n",
    "    fps = video.get(cv2.CAP_PROP_FPS)\n",
    "    print \"Frames per second using video.get(cv2.CAP_PROP_FPS) : {0}\".format(fps)\n",
    "\n",
    "\n",
    "# Number of frames to capture\n",
    "num_frames = 120;\n",
    "\n",
    "\n",
    "print \"Capturing {0} frames\".format(num_frames)\n",
    "\n",
    "# Start time\n",
    "start = time.time()\n",
    "\n",
    "# Grab a few frames\n",
    "for i in xrange(0, num_frames) :\n",
    "    ret, frame = video.read()\n",
    "    cv2.imshow('frame', frame)\n",
    "\n",
    "\n",
    "# End time\n",
    "end = time.time()\n",
    "\n",
    "# Time elapsed\n",
    "seconds = end - start\n",
    "print \"Time taken : {0} seconds\".format(seconds)\n",
    "\n",
    "# Calculate frames per second\n",
    "fps  = num_frames / seconds;\n",
    "print \"Estimated frames per second : {0}\".format(fps);\n",
    "\n",
    "# Release video\n",
    "video.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Capture Video on Camera\n",
    "# Shows video in gray scale with face detection in a new window\n",
    "# Adapted from: https://realpython.com/blog/python/face-recognition-with-python/\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "# Create the haar cascade\n",
    "faceCascade = cv2.CascadeClassifier(\"/usr/local/share/OpenCV/haarcascades/haarcascade_frontalface_default.xml\")\n",
    "\n",
    "while(True):\n",
    "    # Capture frame-by-frame\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    # Our operations on the frame come here\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    # Detect Face in frame\n",
    "    faces = faceCascade.detectMultiScale(\n",
    "    gray,\n",
    "    scaleFactor=1.1,\n",
    "    minNeighbors=5,\n",
    "    minSize=(30, 30)\n",
    "    )\n",
    "    # Draw a rectangle around the faces\n",
    "    for (x, y, w, h) in faces:\n",
    "        cv2.rectangle(gray, (x, y), (x+w, y+h), (0, 255, 0), 2)\n",
    "\n",
    "\n",
    "    # Display the resulting frame\n",
    "    cv2.imshow('frame',gray)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# When everything done, release the capture\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Capture Video on Camera\n",
    "# Shows video in gray scale with face detection in a new window\n",
    "# Adapted from: https://realpython.com/blog/python/face-recognition-with-python/\n",
    "import numpy as np\n",
    "import cv2\n",
    "import dlib\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "win = dlib.image_window()\n",
    "\n",
    "# Create the haar cascade\n",
    "faceCascade = cv2.CascadeClassifier(\"/usr/local/share/OpenCV/haarcascades/haarcascade_frontalface_default.xml\")\n",
    "\n",
    "# Initialize variables for dLib\n",
    "#predictor_path = \"/Users/pybeebee/HS Science Fair/shape_predictor_68_face_landmarks.dat\"\n",
    "#detector = dlib.get_frontal_face_detector()\n",
    "#predictor = dlib.shape_predictor(predictor_path)\n",
    "\n",
    "\n",
    "while(True):\n",
    "    # Capture frame-by-frame\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    # Detect Face in frame\n",
    "    faces = faceCascade.detectMultiScale(frame, scaleFactor=1.1, minNeighbors=5, minSize=(30, 30))\n",
    "    \n",
    "    # Draw a rectangle around the faces\n",
    "    for (x, y, w, h) in faces:\n",
    "        cv2.rectangle(frame, (x, y), (x+w, y+h), (0, 255, 0), 2)\n",
    "    \n",
    "    # Use dLib to locate and display facial landmarks\n",
    "    #dets = detector(frame, 1)\n",
    "    #for k, d in enumerate(dets):\n",
    "        #shape = predictor(frame, d)\n",
    "        #frame.add_overlay(shape)\n",
    "    \n",
    "    # Display the resulting frame\n",
    "    #cv2.imshow('frame',frame) \n",
    "    win.clear_overlay()\n",
    "    win.set_image(frame)\n",
    "    \n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# When everything done, release the capture\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Show video in color\n",
    "# Displays box around face\n",
    "# Displays facial landmarks for each face\n",
    "\n",
    "import cv2\n",
    "import dlib\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# Initialize variables for dLib\n",
    "predictor_path = \"/Users/kaililiu/HS Science Fair/shape_predictor_68_face_landmarks.dat\"\n",
    "detector = dlib.get_frontal_face_detector()\n",
    "predictor = dlib.shape_predictor(predictor_path)\n",
    "\n",
    "while(True):\n",
    "    # Capture frame-by-frame\n",
    "    ret, frame = cap.read()\n",
    "    \n",
    "    # Detect faces and initialize variables for next step\n",
    "    dets = detector(frame, 1)\n",
    "    x_list = list()\n",
    "    y_list = list()\n",
    "    \n",
    "    # Get list of x-coordinates and y-coordinates of facial landmarks for each face\n",
    "    for d in dets:\n",
    "        shape = predictor(frame, d)\n",
    "        for index in range(0, 68):\n",
    "            point = shape.part(index)\n",
    "            x_list.append(point.x)\n",
    "            y_list.append(point.y)\n",
    "    \n",
    "    # Draw a rectangle around the faces\n",
    "    for rect in dets:\n",
    "        cv2.rectangle(frame, (rect.left(), rect.top()), (rect.right(), rect.bottom()), (0, 255, 0), 2)\n",
    "    \n",
    "    # Display facial landmark points on frame\n",
    "    for i in range((len(dets)*68)):\n",
    "        cv2.circle(frame, (x_list[i], y_list[i]), 3, (0, 0, 255), -1)\n",
    "       \n",
    "    # Display frame, press 'q' to exit video window\n",
    "    cv2.imshow('frame',frame)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# When everything done, release the capture\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Show video in color\n",
    "# DOES NOT display box around face\n",
    "# Displays facial landmarks for each face\n",
    "\n",
    "import cv2\n",
    "import dlib\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# Initialize variables for dLib\n",
    "predictor_path = \"/Users/kaililiu/HS Science Fair/shape_predictor_68_face_landmarks.dat\"\n",
    "detector = dlib.get_frontal_face_detector()\n",
    "predictor = dlib.shape_predictor(predictor_path)\n",
    "\n",
    "while(True):\n",
    "    # Capture frame-by-frame\n",
    "    ret, frame = cap.read()\n",
    "    \n",
    "    # Detect faces and initialize variables for next step\n",
    "    dets = detector(frame, 1)\n",
    "    x_list = list()\n",
    "    y_list = list()\n",
    "    \n",
    "    # Get list of x-coordinates and y-coordinates of facial landmarks for each face\n",
    "    for d in dets:\n",
    "        shape = predictor(frame, d)\n",
    "        for index in range(0, 68):\n",
    "            point = shape.part(index)\n",
    "            x_list.append(point.x)\n",
    "            y_list.append(point.y)\n",
    "    \n",
    "    # Display facial landmark points on frame\n",
    "    for i in range((len(dets)*68)):\n",
    "        cv2.circle(frame, (x_list[i], y_list[i]), 4, (0, 255, 0), -1)\n",
    "       \n",
    "    # Display frame, press 'q' to exit video window\n",
    "    cv2.imshow('frame',frame)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# When everything is done, release the capture\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Show video in color\n",
    "# DOES NOT display box around face\n",
    "# Displays facial landmarks for each face, change display upon pressing 'n' or 'o' keys\n",
    "\n",
    "import numpy as np\n",
    "import cv2\n",
    "import dlib\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "state = 1\n",
    "\n",
    "# Initialize variables for dLib\n",
    "predictor_path = \"/Users/kaililiu/HS Science Fair/shape_predictor_68_face_landmarks.dat\"\n",
    "detector = dlib.get_frontal_face_detector()\n",
    "predictor = dlib.shape_predictor(predictor_path)\n",
    "\n",
    "while(True):\n",
    "    # Capture frame-by-frame\n",
    "    ret, frame = cap.read()\n",
    "    origFrame = frame.copy()\n",
    "    \n",
    "    # Detect faces and initialize variables for next step\n",
    "    dets = detector(frame, 1)\n",
    "    x_list = list()\n",
    "    y_list = list()\n",
    "    \n",
    "    # Get list of x-coordinates and y-coordinates of facial landmarks for each face\n",
    "    for d in dets:\n",
    "        shape = predictor(frame, d)\n",
    "        for index in range(0, 68):\n",
    "            point = shape.part(index)\n",
    "            x_list.append(point.x)\n",
    "            y_list.append(point.y)\n",
    "    \n",
    "    # Display facial landmark points on frame\n",
    "    for i in range((len(dets)*68)):\n",
    "        cv2.circle(frame, (x_list[i], y_list[i]), 3, (0, 255, 0), -1)\n",
    "       \n",
    "    # Display frame, n = display with lm, o = display w/o lm, q = exit video window\n",
    "    #cv2.imshow('frame',frame)\n",
    "    key = cv2.waitKey(5)\n",
    "    if key == ord('o'):\n",
    "        state = 0\n",
    "    elif key == ord('l'):\n",
    "        state = 1\n",
    "    elif key == ord('q'):\n",
    "        break\n",
    "        \n",
    "    if state == 0:\n",
    "        cv2.imshow('frame',origFrame)\n",
    "    if state == 1:\n",
    "        cv2.imshow('frame',frame)\n",
    "        \n",
    "\n",
    "# When everything is done, release the capture\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Classifies images in CK+ dataset to either neutral or expression category folders\n",
    "import os\n",
    "\n",
    "for root, dirs, files in os.walk(\"./cohn-kanade-images\"):\n",
    "    filelist=[]\n",
    "    for name in files:\n",
    "        filename = os.path.join(root, name)\n",
    "        if '.png' in filename:\n",
    "            filelist.append(filename)\n",
    "    if len(filelist) > 0:\n",
    "        neutralfile = filelist[0]\n",
    "        !cp $neutralfile Neutral\n",
    "        expressfile = filelist[-1]\n",
    "        !cp $expressfile Expression\n",
    "        print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Apply 68-point Facial Landmark Detection Model to CK+ Dataset, neutral faces\n",
    "# DOES display box around face\n",
    "# Displays facial landmarks for each face\n",
    "\n",
    "import cv2\n",
    "import dlib\n",
    "import os\n",
    "\n",
    "# Initialize variables for dLib\n",
    "predictor_path = \"/Users/kaililiu/HS Science Fair/shape_predictor_68_face_landmarks.dat\"\n",
    "detector = dlib.get_frontal_face_detector()\n",
    "predictor = dlib.shape_predictor(predictor_path)\n",
    "\n",
    "for root, dirs, files in os.walk(\"./Neutral\"):\n",
    "    for name in files:\n",
    "        filename = os.path.join(root, name)\n",
    "\n",
    "        # Read image if of correct type (8bit gray or RGB)\n",
    "        if '.png' in filename:\n",
    "            image = cv2.imread(filename, 1)\n",
    "\n",
    "            # Detect faces and initialize variables for next step\n",
    "            dets = detector(image, 1)\n",
    "            x_list = list()\n",
    "            y_list = list()\n",
    "\n",
    "            # Get list of x-coordinates and y-coordinates of facial landmarks for each face\n",
    "            for d in dets:\n",
    "                shape = predictor(image, d)\n",
    "                for index in range(0, 68):\n",
    "                    point = shape.part(index)\n",
    "                    x_list.append(point.x)\n",
    "                    y_list.append(point.y)\n",
    "\n",
    "            # Display rectangle around face\n",
    "            for rect in dets:\n",
    "                cv2.rectangle(image, (rect.left(), rect.top()), (rect.right(), rect.bottom()), (0, 255, 0), 2)\n",
    "            \n",
    "            # Display facial landmark points on frame\n",
    "            for i in range((len(dets)*68)):\n",
    "                cv2.circle(image, (x_list[i], y_list[i]), 3, (0, 255, 0), -1)\n",
    "\n",
    "            # Display frame, 'q' to stop\n",
    "            cv2.imshow('frame',image)\n",
    "            key = cv2.waitKey(50)\n",
    "            if key == ord('q'):\n",
    "                break\n",
    "\n",
    "# When everything is done, destroy the window\n",
    "#cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Apply 68-point Facial Landmark Detection Model to CK+ Dataset, faces with expressions\n",
    "# DOES display box around face\n",
    "# Displays facial landmarks for each face\n",
    "\n",
    "import cv2\n",
    "import dlib\n",
    "import os\n",
    "\n",
    "# Initialize variables for dLib\n",
    "predictor_path = \"/Users/kaililiu/HS Science Fair/shape_predictor_68_face_landmarks.dat\"\n",
    "detector = dlib.get_frontal_face_detector()\n",
    "predictor = dlib.shape_predictor(predictor_path)\n",
    "\n",
    "for root, dirs, files in os.walk(\"./Expression\"):\n",
    "    for name in files:\n",
    "        filename = os.path.join(root, name)\n",
    "\n",
    "        # Read image if of correct type (8bit gray or RGB)\n",
    "        if '.png' in filename:\n",
    "            image = cv2.imread(filename, 1)\n",
    "\n",
    "            # Detect faces and initialize variables for next step\n",
    "            dets = detector(image, 1)\n",
    "            x_list = list()\n",
    "            y_list = list()\n",
    "\n",
    "            # Get list of x-coordinates and y-coordinates of facial landmarks for each face\n",
    "            for d in dets:\n",
    "                shape = predictor(image, d)\n",
    "                for index in range(0, 68):\n",
    "                    point = shape.part(index)\n",
    "                    x_list.append(point.x)\n",
    "                    y_list.append(point.y)\n",
    "\n",
    "            # Display rectangle around face\n",
    "            for rect in dets:\n",
    "                cv2.rectangle(image, (rect.left(), rect.top()), (rect.right(), rect.bottom()), (0, 255, 0), 2)\n",
    "            \n",
    "            # Display facial landmark points on frame\n",
    "            for i in range((len(dets)*68)):\n",
    "                cv2.circle(image, (x_list[i], y_list[i]), 3, (0, 255, 0), -1)\n",
    "\n",
    "            # Display frame, 'q' to stop\n",
    "            cv2.imshow('frame',image)\n",
    "            key = cv2.waitKey(50)\n",
    "            if key == ord('q'):\n",
    "                break\n",
    "\n",
    "# When everything is done, destroy the window\n",
    "#cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# OLD: get numpy array of expression and neutral differences for each image\n",
    "import cv2\n",
    "import dlib\n",
    "import os\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "# Initialize variables for dLib\n",
    "predictor_path = \"/Users/kaililiu/HS Science Fair/shape_predictor_68_face_landmarks.dat\"\n",
    "detector = dlib.get_frontal_face_detector()\n",
    "predictor = dlib.shape_predictor(predictor_path)\n",
    "# Initialize other variables\n",
    "NLA = []\n",
    "ELA = []\n",
    "tempLDA = []\n",
    "# Get landmarks for neutral images\n",
    "for root, dirs, files in os.walk(\"./Neutral\"):\n",
    "    for name in files:\n",
    "        filename = os.path.join(root, name)\n",
    "\n",
    "        # Read image if of correct type (8bit gray or RGB)\n",
    "        if '.png' in filename:\n",
    "            image = cv2.imread(filename, 1)\n",
    "\n",
    "            # Detect faces and initialize variables for next step\n",
    "            dets = detector(image, 1)\n",
    "\n",
    "            # Get list of x-coordinates and y-coordinates of facial landmarks for each face\n",
    "            for d in dets:\n",
    "                imageNA = []\n",
    "                shape = predictor(image, d)\n",
    "                for index in range(0, 68):\n",
    "                    point = shape.part(index)\n",
    "                    tuplepoint = (point.x, point.y)\n",
    "                    imageNA.append(tuplepoint)\n",
    "                NLA.append(imageNA)\n",
    "                \n",
    "# Get landmarks for expression images\n",
    "for root, dirs, files in os.walk(\"./Expression\"):\n",
    "    for name in files:\n",
    "        filename = os.path.join(root, name)\n",
    "\n",
    "        # Read image if of correct type (8bit gray or RGB)\n",
    "        if '.png' in filename:\n",
    "            image = cv2.imread(filename, 1)\n",
    "\n",
    "            # Detect faces and initialize variables for next step\n",
    "            dets = detector(image, 1)\n",
    "\n",
    "            # Get list of x-coordinates and y-coordinates of facial landmarks for each face\n",
    "            for d in dets:\n",
    "                imageEA = []\n",
    "                shape = predictor(image, d)\n",
    "                for index in range(0, 68):\n",
    "                    point = shape.part(index)\n",
    "                    tuplepoint = (point.x, point.y)\n",
    "                    imageEA.append(tuplepoint)\n",
    "                ELA.append(imageEA)\n",
    "\n",
    "# Get list of Euclidean differences for each landmark point for each image, convert list to numpy.arrray, add to big list\n",
    "for img in range(len(NLA)):\n",
    "    tempImgLDA = []\n",
    "    for point in range(68):\n",
    "        dif = math.sqrt((math.pow(ELA[img][point][0]-NLA[img][point][0], 2)) + (math.pow(ELA[img][point][1]-NLA[img][point][1], 2)))\n",
    "        tempImgLDA.append(dif)\n",
    "    imgLDA = np.asarray(tempImgLDA)\n",
    "    tempLDA.append(imgLDA)\n",
    "\n",
    "# Convert big list of all differences for all images to numpy.array\n",
    "LDA = np.asarray(tempLDA)\n",
    "\n",
    "# Subtract the average from the LDA values to see if there is any change\n",
    "for img in range(len(LDA)):\n",
    "    sum = 0\n",
    "    for valueIndex in range(len(LDA[img])):\n",
    "        sum+=LDA[img][valueIndex]\n",
    "    average = sum/68\n",
    "    for vI in range(len(LDA[img])):\n",
    "        LDA[img][vI]-=average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# DRAFT\n",
    "# Create 327x3 matrix, for each image set contains neutral image name, expression image name, emotion label\n",
    "# Generate normalized Euclidean Differences array\n",
    "# Generate target values array\n",
    "# Feed to SVM, evaulate with Cross Validation\n",
    "import cv2\n",
    "import dlib\n",
    "import os\n",
    "import math\n",
    "\n",
    "# Function that returns array of 68 landmarks for given image name\n",
    "def get68LMA(imgName):\n",
    "    predictor_path = \"/Users/kaililiu/HS Science Fair/shape_predictor_68_face_landmarks.dat\"\n",
    "    detector = dlib.get_frontal_face_detector()\n",
    "    predictor = dlib.shape_predictor(predictor_path)\n",
    "    LMA = []\n",
    "    image = cv2.imread(imgName, 1)\n",
    "    \n",
    "    # Detect face(s)\n",
    "    dets = detector(image, 1)\n",
    "\n",
    "    # Get list of coordinates of 68 facial landmarks for each face\n",
    "    for d in dets:\n",
    "        shape = predictor(image, d)\n",
    "        for index in range(0, 68):\n",
    "            point = shape.part(index)\n",
    "            tuplepoint = (point.x, point.y)\n",
    "            LMA.append(tuplepoint)\n",
    "    return LMA\n",
    "\n",
    "# Function that eturns normalized array of Euclidean distances given two arrays of coordinates\n",
    "def getEucDist(nLA, eLA):\n",
    "    LDA = []\n",
    "    sum = 0\n",
    "    # Get array of distances\n",
    "    for pt in range(68):\n",
    "        x1 = eLA[pt][0]\n",
    "        x2 = nLA[pt][0]\n",
    "        y1 = eLA[pt][1]\n",
    "        y2 = nLA[pt][1]\n",
    "        dif = math.sqrt((x1-x2)*(x1-x2) + (y1-y2)*(y1-y2))     \n",
    "        LDA.append(dif)\n",
    "        sum += dif\n",
    "    avg = sum/68\n",
    "    # Normalize array\n",
    "    for i in range(68):\n",
    "        LDA[i] -= avg\n",
    "        \n",
    "    return LDA\n",
    "\n",
    "\n",
    "# Get paths to face images and emotion labels\n",
    "imagedir = []\n",
    "labeldir = []\n",
    "root, dirs, files = os.walk(\"./cohn-kanade-images\",).next()\n",
    "for d in dirs:\n",
    "    imagedir.append(os.path.join(root,d))\n",
    "    labeldir.append(os.path.join('./Emotion',d))\n",
    "\n",
    "# initialize datset\n",
    "dataset = []\n",
    "\n",
    "# look over all directories of individuals S001 through S999\n",
    "for d in dirs:\n",
    "    for dd in os.listdir(os.path.join('./cohn-kanade-images',d)):\n",
    "        # skip over .DS_Store files\n",
    "        if 'Store' not in dd:\n",
    "            curdir = os.path.join('./cohn-kanade-images',d,dd)\n",
    "            files = os.listdir(curdir)\n",
    "            baseimage = os.path.join(curdir,files[0])\n",
    "            faceimage = os.path.join(curdir,files[-1])\n",
    "\n",
    "            # get directories of multiple expressions of same person\n",
    "            labeldir = os.path.join('./Emotion',d,dd)\n",
    "            if os.path.isdir(labeldir):\n",
    "                labels = os.listdir(labeldir)\n",
    "                # read label file if it exists\n",
    "                if len(labels)>0:\n",
    "                    f=open(os.path.join(labeldir,labels[0]),\"r\")\n",
    "                    for line in f:\n",
    "                        targetlabel = float(line)\n",
    "                    f.close()\n",
    "                    \n",
    "                    # append label to dataset\n",
    "                    dataset.append([baseimage, faceimage, targetlabel])\n",
    "\n",
    "# Get array of Euclidean differences between neutral and expression 68-landmark arrays\n",
    "LDA = []\n",
    "for imgIndex in range(len(dataset)):\n",
    "    nImgName = dataset[imgIndex][0]\n",
    "    eImgName = dataset[imgIndex][1]\n",
    "    nLA = get68LMA(nImgName)\n",
    "    eLA = get68LMA(eImgName)\n",
    "    LDA.append(getEucDist(nLA, eLA))\n",
    "\n",
    "# Create target value array for expression images\n",
    "targetValues = []\n",
    "for imgSet in range(len(dataset)):\n",
    "    targetValues.append(dataset[imgSet][2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# DRAFT\n",
    "import numpy as np\n",
    "realLDA = np.asmatrix(LDA)\n",
    "np.savetxt('LDAfile', realLDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# DRAFT\n",
    "realTargetValues = np.asarray(targetValues)\n",
    "np.savetxt('targetValuesFile', realTargetValues)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.70588235  0.70731707  0.69135802  0.70886076]\n"
     ]
    }
   ],
   "source": [
    "# OLD: use cross validation\n",
    "from sklearn import svm\n",
    "from sklearn import cross_validation\n",
    "clf = svm.SVC(kernel='linear').fit(realLDA, realTargetValues)\n",
    "scores = cross_validation.cross_val_score(clf, realLDA, realTargetValues, cv=4)\n",
    "print scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 60.    0.   15.8  15.4   0.   25.    3.2]\n",
      " [  0.   62.5   5.3   0.    3.1  12.5   0. ]\n",
      " [ 20.   12.5  57.9  15.4   0.   12.5   0. ]\n",
      " [ 15.    0.   15.8  38.5   3.1  12.5   0. ]\n",
      " [  0.    0.   10.5   7.7  90.6   0.    0. ]\n",
      " [  5.   25.    0.    0.    3.1  50.    0. ]\n",
      " [ 15.    0.    0.   23.1   0.    0.   80.6]]\n"
     ]
    }
   ],
   "source": [
    "# Calculate confusion matrix for base model with SVM and Cross Validation\n",
    "# Convert confusion matrix to contain percentages \n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "x_train, x_test, y_train, y_test = cross_validation.train_test_split(realLDA, realTargetValues, test_size=0.4, random_state=2)\n",
    "clf2 = svm.SVC(kernel='linear').fit(x_train, y_train)\n",
    "predictions = clf2.predict(x_test)\n",
    "cmatrix = confusion_matrix(y_test, predictions)\n",
    "\n",
    "# Format CMatrix to have percentages\n",
    "np.set_printoptions(precision=1)\n",
    "rowSums = np.sum(cmatrix, axis=1)\n",
    "pCMatrix = np.divide(cmatrix*100., np.sum(cmatrix, axis=1), dtype=float)\n",
    "print pCMatrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 60.    0.   15.8  15.4   0.   25.    3.2]\n",
      " [  0.   62.5   5.3   0.    3.1  12.5   0. ]\n",
      " [ 20.   12.5  57.9  15.4   0.   12.5   0. ]\n",
      " [ 15.    0.   15.8  38.5   3.1  12.5   0. ]\n",
      " [  0.    0.   10.5   7.7  90.6   0.    0. ]\n",
      " [  5.   25.    0.    0.    3.1  50.    0. ]\n",
      " [ 15.    0.    0.   23.1   0.    0.   80.6]]\n"
     ]
    }
   ],
   "source": [
    "# WORKING VERSION OF COMBINED DRAFTS\n",
    "# Create 327x3 matrix, for each image set contains neutral image name, expression image name, emotion label\n",
    "# Generate normalized Euclidean Differences array\n",
    "# Generate target values array\n",
    "# Feed to SVM, evaulate with Cross Validation\n",
    "import numpy as np\n",
    "import cv2\n",
    "import dlib\n",
    "import os\n",
    "import math\n",
    "from sklearn import svm\n",
    "from sklearn import cross_validation\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Function that returns array of 68 landmarks for given image name\n",
    "def get68LMA(imgName):\n",
    "    predictor_path = \"/Users/kaililiu/HS Science Fair/shape_predictor_68_face_landmarks.dat\"\n",
    "    detector = dlib.get_frontal_face_detector()\n",
    "    predictor = dlib.shape_predictor(predictor_path)\n",
    "    LMA = []\n",
    "    image = cv2.imread(imgName, 1)\n",
    "    \n",
    "    # Detect face(s)\n",
    "    dets = detector(image, 1)\n",
    "\n",
    "    # Get list of coordinates of 68 facial landmarks for each face\n",
    "    for d in dets:\n",
    "        shape = predictor(image, d)\n",
    "        for index in range(0, 68):\n",
    "            point = shape.part(index)\n",
    "            tuplepoint = (point.x, point.y)\n",
    "            LMA.append(tuplepoint)\n",
    "    return LMA\n",
    "\n",
    "# Function that eturns normalized array of Euclidean distances given two arrays of coordinates\n",
    "def getEucDist(nLA, eLA):\n",
    "    LDA = []\n",
    "    sum = 0\n",
    "    # Get array of distances\n",
    "    for pt in range(68):\n",
    "        x1 = eLA[pt][0]\n",
    "        x2 = nLA[pt][0]\n",
    "        y1 = eLA[pt][1]\n",
    "        y2 = nLA[pt][1]\n",
    "        dif = math.sqrt((x1-x2)*(x1-x2) + (y1-y2)*(y1-y2))     \n",
    "        LDA.append(dif)\n",
    "        sum += dif\n",
    "    avg = sum/68\n",
    "    # Normalize array\n",
    "    for i in range(68):\n",
    "        LDA[i] -= avg\n",
    "        \n",
    "    return LDA\n",
    "\n",
    "# Get paths to face images and emotion labels\n",
    "imagedir = []\n",
    "labeldir = []\n",
    "root, dirs, files = os.walk(\"./cohn-kanade-images\",).next()\n",
    "for d in dirs:\n",
    "    imagedir.append(os.path.join(root,d))\n",
    "    labeldir.append(os.path.join('./Emotion',d))\n",
    "\n",
    "# initialize datset\n",
    "dataset = []\n",
    "\n",
    "# look over all directories of individuals S001 through S999\n",
    "for d in dirs:\n",
    "    for dd in os.listdir(os.path.join('./cohn-kanade-images',d)):\n",
    "        # skip over .DS_Store files\n",
    "        if 'Store' not in dd:\n",
    "            curdir = os.path.join('./cohn-kanade-images',d,dd)\n",
    "            files = os.listdir(curdir)\n",
    "            baseimage = os.path.join(curdir,files[0])\n",
    "            faceimage = os.path.join(curdir,files[-1])\n",
    "\n",
    "            # get directories of multiple expressions of same person\n",
    "            labeldir = os.path.join('./Emotion',d,dd)\n",
    "            if os.path.isdir(labeldir):\n",
    "                labels = os.listdir(labeldir)\n",
    "                # read label file if it exists\n",
    "                if len(labels)>0:\n",
    "                    f=open(os.path.join(labeldir,labels[0]),\"r\")\n",
    "                    for line in f:\n",
    "                        targetlabel = float(line)\n",
    "                    f.close()\n",
    "                    \n",
    "                    # append label to dataset\n",
    "                    dataset.append([baseimage, faceimage, targetlabel])\n",
    "\n",
    "# Get array of Euclidean differences between neutral and expression 68-landmark arrays\n",
    "tempLDA = []\n",
    "for imgIndex in range(len(dataset)):\n",
    "    nImgName = dataset[imgIndex][0]\n",
    "    eImgName = dataset[imgIndex][1]\n",
    "    nLA = get68LMA(nImgName)\n",
    "    eLA = get68LMA(eImgName)\n",
    "    tempLDA.append(getEucDist(nLA, eLA))\n",
    "LDA = np.asmatrix(tempLDA)\n",
    "\n",
    "# Create target value array for expression images\n",
    "tempTargetValues = []\n",
    "for imgSet in range(len(dataset)):\n",
    "    tempTargetValues.append(dataset[imgSet][2])\n",
    "targetValues = np.asarray(tempTargetValues)\n",
    "\n",
    "# Feed LDA and targetValues int SVM, use cross validation, generate confusion matrix\n",
    "x_train, x_test, y_train, y_test = cross_validation.train_test_split(LDA, targetValues, test_size=0.4, random_state=2)\n",
    "clf = svm.SVC(kernel='linear').fit(x_train, y_train)\n",
    "predictions = clf.predict(x_test)\n",
    "cmatrix = confusion_matrix(y_test, predictions)\n",
    "\n",
    "# Format CMatrix to show percentages, rounded to 1 decimal point\n",
    "np.set_printoptions(precision=1)\n",
    "rowSums = np.sum(cmatrix, axis=1)\n",
    "pCMatrix = np.divide(cmatrix*100., np.sum(cmatrix, axis=1), dtype=float)\n",
    "print pCMatrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 60.    0.   15.8  15.4   0.   25.    3.2]\n",
      " [  0.   62.5   5.3   0.    3.1  12.5   0. ]\n",
      " [ 20.   12.5  57.9  15.4   0.   12.5   0. ]\n",
      " [ 15.    0.   15.8  38.5   3.1  12.5   0. ]\n",
      " [  0.    0.   10.5   7.7  90.6   0.    0. ]\n",
      " [  5.   25.    0.    0.    3.1  50.    0. ]\n",
      " [ 15.    0.    0.   23.1   0.    0.   80.6]]\n",
      "[[ 73.3  28.6  10.    0.    0.    0.    0. ]\n",
      " [  6.7  85.7   0.    0.    0.    0.    0. ]\n",
      " [  6.7  14.3  90.    0.    0.    0.    0. ]\n",
      " [  6.7   0.    0.   63.6   3.6  13.3   0. ]\n",
      " [  0.    0.    5.    0.   96.4   0.    0. ]\n",
      " [ 33.3   0.    0.   27.3   0.   40.    2.9]\n",
      " [  0.   14.3   0.    9.1   0.    6.7  91.4]]\n"
     ]
    }
   ],
   "source": [
    "# WORKING VERSION OF COMBINED DRAFTS\n",
    "# Create 327x3 matrix, for each image set contains neutral image name, expression image name, emotion label\n",
    "# Generate normalized Euclidean Differences array\n",
    "# Generate target values array\n",
    "# Feed to SVM, evaulate with Cross Validation\n",
    "import numpy as np\n",
    "import cv2\n",
    "import dlib\n",
    "import os\n",
    "import math\n",
    "from sklearn import svm\n",
    "from sklearn import cross_validation\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Function that returns the distance between two given points\n",
    "def ptDist(pt1, pt2):\n",
    "    x1 = pt1[0]\n",
    "    x2 = pt2[0]\n",
    "    y1 = pt1[1]\n",
    "    y2 = pt2[1]\n",
    "    return math.sqrt((x1-x2)*(x1-x2) + (y1-y2)*(y1-y2))\n",
    "\n",
    "# Function that returns array of 68 landmarks for given image name\n",
    "def get68LMA(imgName):\n",
    "    predictor_path = \"/Users/kaililiu/HS Science Fair/shape_predictor_68_face_landmarks.dat\"\n",
    "    detector = dlib.get_frontal_face_detector()\n",
    "    predictor = dlib.shape_predictor(predictor_path)\n",
    "    LMA = []\n",
    "    image = cv2.imread(imgName, 1)\n",
    "    \n",
    "    # Detect face(s)\n",
    "    dets = detector(image, 1)\n",
    "\n",
    "    # Get list of coordinates of 68 facial landmarks for each face\n",
    "    for d in dets:\n",
    "        shape = predictor(image, d)\n",
    "        for index in range(0, 68):\n",
    "            point = shape.part(index)\n",
    "            tuplepoint = (point.x, point.y)\n",
    "            LMA.append(tuplepoint)\n",
    "    return LMA\n",
    "\n",
    "# Function that eturns normalized array of Euclidean distances given two arrays of coordinates\n",
    "def getEucDist(nLA, eLA):\n",
    "    LDA = []\n",
    "    sum = 0\n",
    "    # Get array of distances\n",
    "    for pt in range(68):\n",
    "        dif = ptDist(eLA[pt], nLA[pt])     \n",
    "        LDA.append(dif)\n",
    "        sum += dif\n",
    "    avg = sum/68\n",
    "    # Normalize array\n",
    "    for i in range(68):\n",
    "        LDA[i] -= avg      \n",
    "    return LDA\n",
    "\n",
    "# Function that returns 13-point numpy vector of distances between selected physiological pts, given the 68-point vector\n",
    "def getPDA(iLA):\n",
    "    PDA = []\n",
    "    PDA.append(ptDist(iLA[20], iLA[21])) #distance 0: distance between brows\n",
    "    PDA.append(ptDist(iLA[20], iLA[26])) #distance 1: inner brow to nose- left\n",
    "    PDA.append(ptDist(iLA[21], iLA[26])) #distance 2: inner brow to nose- right\n",
    "    PDA.append(ptDist(iLA[20], iLA[38])) #distance 3: inner brow to inner eye corner- left\n",
    "    PDA.append(ptDist(iLA[21], iLA[41])) #distance 4: inner brow to inner eye corner- right\n",
    "    PDA.append(ptDist(iLA[35], iLA[47])) #distance 5: outer eye corner to outer mouth corner- left\n",
    "    PDA.append(ptDist(iLA[44], iLA[53])) #distance 6: outer eye corner to outer mouth corner- right\n",
    "    PDA.append(ptDist(iLA[47], iLA[53])) #distance 7: distance between mouth corners\n",
    "    PDA.append(ptDist(iLA[61], iLA[65])) #distance 8: height of mouth\n",
    "    PDA.append(ptDist(iLA[35], iLA[49])) #distance 9: raising of upper lip\n",
    "    PDA.append(ptDist(iLA[44], iLA[51])) #distance 10: raising of upper lip\n",
    "    PDA.append(ptDist(iLA[18], iLA[36])) #distance 11: brow arch- left\n",
    "    PDA.append(ptDist(iLA[23], iLA[43])) #distance 12: brow arch- right\n",
    "    return np.asarray(PDA)\n",
    "\n",
    "\"\"\"MAIN PROGRAM BEGINS\"\"\"\n",
    "# Get paths to face images and emotion labels\n",
    "imagedir = []\n",
    "labeldir = []\n",
    "root, dirs, files = os.walk(\"./cohn-kanade-images\",).next()\n",
    "for d in dirs:\n",
    "    imagedir.append(os.path.join(root,d))\n",
    "    labeldir.append(os.path.join('./Emotion',d))\n",
    "\n",
    "# initialize variables\n",
    "dataset = []\n",
    "NLA = []\n",
    "ELA = []\n",
    "\n",
    "# look over all directories of individuals S001 through S999\n",
    "for d in dirs:\n",
    "    for dd in os.listdir(os.path.join('./cohn-kanade-images',d)):\n",
    "        # skip over .DS_Store files\n",
    "        if 'Store' not in dd:\n",
    "            curdir = os.path.join('./cohn-kanade-images',d,dd)\n",
    "            files = os.listdir(curdir)\n",
    "            baseimage = os.path.join(curdir,files[0])\n",
    "            faceimage = os.path.join(curdir,files[-1])\n",
    "\n",
    "            # get directories of multiple expressions of same person\n",
    "            labeldir = os.path.join('./Emotion',d,dd)\n",
    "            if os.path.isdir(labeldir):\n",
    "                labels = os.listdir(labeldir)\n",
    "                # read label file if it exists\n",
    "                if len(labels)>0:\n",
    "                    f=open(os.path.join(labeldir,labels[0]),\"r\")\n",
    "                    for line in f:\n",
    "                        targetlabel = float(line)\n",
    "                    f.close()\n",
    "                    \n",
    "                    # append label to dataset\n",
    "                    dataset.append([baseimage, faceimage, targetlabel])\n",
    "\n",
    "\"\"\"68-POINT LANDMARK DETECTION MODEL\"\"\"\n",
    "# Get array of Euclidean differences between neutral and expression 68-landmark arrays\n",
    "tempLDA = []\n",
    "for imgIndex in range(len(dataset)):\n",
    "    nImgName = dataset[imgIndex][0]\n",
    "    eImgName = dataset[imgIndex][1]\n",
    "    nLA = get68LMA(nImgName)\n",
    "    eLA = get68LMA(eImgName)\n",
    "    NLA.append(nLA)\n",
    "    ELA.append(eLA)\n",
    "    tempLDA.append(getEucDist(nLA, eLA))\n",
    "LDA = np.asmatrix(tempLDA)\n",
    "\n",
    "# Create target value array for expression images\n",
    "tempTargetValues = []\n",
    "for imgSet in range(len(dataset)):\n",
    "    tempTargetValues.append(dataset[imgSet][2])\n",
    "targetValues = np.asarray(tempTargetValues)\n",
    "\n",
    "# Feed LDA and targetValues into SVM, use cross validation, generate confusion matrix\n",
    "x_train, x_test, y_train, y_test = cross_validation.train_test_split(LDA, targetValues, test_size=0.4, random_state=2)\n",
    "normCLF = svm.SVC(kernel='linear').fit(x_train, y_train)\n",
    "predictions = normCLF.predict(x_test)\n",
    "cmatrix = confusion_matrix(y_test, predictions)\n",
    "\n",
    "# Format CMatrix to show percentages, rounded to 1 decimal point\n",
    "np.set_printoptions(precision=1)\n",
    "rowSums = np.sum(cmatrix, axis=1)\n",
    "pCMatrix = np.divide(cmatrix*100., rowSums, dtype=float)\n",
    "print pCMatrix\n",
    "\n",
    "\n",
    "\"\"\"PHYSIOLOGICALLY BASED LANDMARK DETECTION MODEL\"\"\"\n",
    "# Get array of corresponding differences between 13 distances of base images and expression images\n",
    "tempPDA = []\n",
    "for i in range(len(NLA)):\n",
    "    tempPDA.append(getPDA(NLA[i])-getPDA(ELA[i]))\n",
    "PDA = np.asmatrix(tempPDA)\n",
    "np.savetxt('PDAfile', PDA) #Save PDA as human-readable format file\n",
    "\n",
    "# Feed PDA and targetValues into SVM, use cross validation, generate confusion matrix\n",
    "x_train2, x_test2, y_train2, y_test2 = cross_validation.train_test_split(PDA, targetValues, test_size=0.4, random_state=3)\n",
    "physCLF = svm.SVC(kernel='linear').fit(x_train2, y_train2)\n",
    "predictions2 = physCLF.predict(x_test2)\n",
    "cMatrix2 = confusion_matrix(y_test2, predictions2)\n",
    "\n",
    "# Format CMatrix2 to show percentages, already rounded to 1 decimal point\n",
    "rowSums2 = np.sum(cMatrix2, axis=1)\n",
    "pCMatrix2 = np.divide(cMatrix2*100., rowSums2, dtype=float)\n",
    "print pCMatrix2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.703354552574\n",
      "0.840391634453\n"
     ]
    }
   ],
   "source": [
    "np.set_printoptions(precision=3)\n",
    "normModelScores = cross_validation.cross_val_score(normCLF, LDA, targetValues, cv=4)\n",
    "physModelScores = cross_validation.cross_val_score(normCLF, PDA, targetValues, cv=4)\n",
    "print np.average(normModelScores)\n",
    "print np.average(physModelScores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[15  0  4  0  0  1  0]\n",
      " [ 0  7  0  0  0  1  0]\n",
      " [ 0  0 19  0  0  0  0]\n",
      " [ 1  1  0  8  1  2  0]\n",
      " [ 0  0  0  0 32  0  0]\n",
      " [ 1  1  1  0  0  4  1]\n",
      " [ 0  0  0  2  0  1 28]]\n",
      "[[  55.      25.      10.526    0.       0.       0.       0.   ]\n",
      " [   5.      75.       0.       0.       0.       0.       0.   ]\n",
      " [   5.      12.5     94.737    0.       0.       0.       0.   ]\n",
      " [   5.       0.       0.      53.846    3.125   25.       0.   ]\n",
      " [   0.       0.       5.263    0.      84.375    0.       0.   ]\n",
      " [  25.       0.       0.      23.077    0.      75.       3.226]\n",
      " [   0.      12.5      0.       7.692    0.      12.5    103.226]]\n",
      "[20  8 19 13 32  8 31]\n",
      "0.903225806452\n"
     ]
    }
   ],
   "source": [
    "x_train3, x_test3, y_train3, y_test3 = cross_validation.train_test_split(PDA, targetValues, test_size=0.4, random_state=2)\n",
    "physCLF = svm.SVC(kernel='linear').fit(x_train3, y_train3)\n",
    "predictions3 = physCLF.predict(x_test3)\n",
    "cMatrix3 = confusion_matrix(y_test3, predictions3)\n",
    "print cMatrix3\n",
    "# Format CMatrix2 to show percentages, already rounded to 1 decimal point\n",
    "rowSums3 = np.sum(cMatrix3, axis=1)\n",
    "pCMatrix3 = np.divide(cMatrix2*100., np.sum(cMatrix3, axis=1), dtype=float)\n",
    "print pCMatrix3\n",
    "print rowSums3\n",
    "print 28./31."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Show video in color\n",
    "# Displays box around face\n",
    "# Displays facial landmarks for each face\n",
    "\n",
    "import cv2\n",
    "import dlib\n",
    "\n",
    "# Initialize variables for dLib\n",
    "predictor_path = \"/Users/kaililiu/HS Science Fair/shape_predictor_68_face_landmarks.dat\"\n",
    "detector = dlib.get_frontal_face_detector()\n",
    "predictor = dlib.shape_predictor(predictor_path)\n",
    "\n",
    "while(True):\n",
    "    # Capture frame-by-frame\n",
    "    image = cv2.imread('',1)\n",
    "    \n",
    "    # Detect faces and initialize variables for next step\n",
    "    dets = detector(image, 1)\n",
    "    x_list = list()\n",
    "    y_list = list()\n",
    "    \n",
    "    # Get list of x-coordinates and y-coordinates of facial landmarks for each face\n",
    "    for d in dets:\n",
    "        shape = predictor(frame, d)\n",
    "        for index in range(0, 68):\n",
    "            point = shape.part(index)\n",
    "            x_list.append(point.x)\n",
    "            y_list.append(point.y)\n",
    "    \n",
    "    # Draw a rectangle around the faces\n",
    "    for rect in dets:\n",
    "        cv2.rectangle(frame, (rect.left(), rect.top()), (rect.right(), rect.bottom()), (0, 255, 0), 2)\n",
    "    \n",
    "    # Display facial landmark points on frame\n",
    "    for i in range((len(dets)*68)):\n",
    "        cv2.circle(frame, (x_list[i], y_list[i]), 3, (0, 0, 255), -1)\n",
    "       \n",
    "    # Display frame, press 'q' to exit video window\n",
    "    cv2.imshow('frame',image)\n",
    "\n",
    "# When everything done, release the capture\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
