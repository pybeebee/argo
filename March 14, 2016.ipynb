{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "print cv2.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Capture Video on Camera\n",
    "# Shows video in gray scale in a new window\n",
    "# Adapted from: http://docs.opencv.org/3.0-beta/doc/py_tutorials/py_gui/py_video_display/py_video_display.html\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "while(True):\n",
    "    # Capture frame-by-frame\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    # Our operations on the frame come here\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Display the resulting frame\n",
    "    cv2.imshow('frame',gray)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# When everything done, release the capture\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Capture Video on Camera\n",
    "# Shows video in gray scale in a new window\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "cap = cv2.VideoCapture()\n",
    "\n",
    "while(True):\n",
    "    # Capture frame-by-frame\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    # Our operations on the frame come here\n",
    "    #gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Display the resulting frame\n",
    "    cv2.imshow('frame', frame)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# When everything done, release the capture\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Calculates Frames Per Second of Video/Camera\n",
    "# Adapted from: http://www.learnopencv.com/how-to-find-frame-rate-or-frames-per-second-fps-in-opencv-python-cpp/\n",
    "import cv2\n",
    "import time\n",
    " \n",
    "# Start default camera\n",
    "video = cv2.VideoCapture(0);\n",
    "\n",
    "# Find OpenCV version\n",
    "(major_ver, minor_ver, subminor_ver) = (cv2.__version__).split('.')\n",
    "\n",
    "# With webcam get(CV_CAP_PROP_FPS) does not work.\n",
    "# Let's see for ourselves.\n",
    "\n",
    "if int(major_ver)  < 3 :\n",
    "    fps = video.get(cv2.cv.CV_CAP_PROP_FPS)\n",
    "    print \"Frames per second using video.get(cv2.cv.CV_CAP_PROP_FPS): {0}\".format(fps)\n",
    "else :\n",
    "    fps = video.get(cv2.CAP_PROP_FPS)\n",
    "    print \"Frames per second using video.get(cv2.CAP_PROP_FPS) : {0}\".format(fps)\n",
    "\n",
    "\n",
    "# Number of frames to capture\n",
    "num_frames = 120;\n",
    "\n",
    "\n",
    "print \"Capturing {0} frames\".format(num_frames)\n",
    "\n",
    "# Start time\n",
    "start = time.time()\n",
    "\n",
    "# Grab a few frames\n",
    "for i in xrange(0, num_frames) :\n",
    "    ret, frame = video.read()\n",
    "    cv2.imshow('frame', frame)\n",
    "\n",
    "\n",
    "# End time\n",
    "end = time.time()\n",
    "\n",
    "# Time elapsed\n",
    "seconds = end - start\n",
    "print \"Time taken : {0} seconds\".format(seconds)\n",
    "\n",
    "# Calculate frames per second\n",
    "fps  = num_frames / seconds;\n",
    "print \"Estimated frames per second : {0}\".format(fps);\n",
    "\n",
    "# Release video\n",
    "video.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Capture Video on Camera\n",
    "# Shows video in gray scale with face detection in a new window\n",
    "# Adapted from: https://realpython.com/blog/python/face-recognition-with-python/\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "# Create the haar cascade\n",
    "faceCascade = cv2.CascadeClassifier(\"/usr/local/share/OpenCV/haarcascades/haarcascade_frontalface_default.xml\")\n",
    "\n",
    "while(True):\n",
    "    # Capture frame-by-frame\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    # Our operations on the frame come here\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    # Detect Face in frame\n",
    "    faces = faceCascade.detectMultiScale(\n",
    "    gray,\n",
    "    scaleFactor=1.1,\n",
    "    minNeighbors=5,\n",
    "    minSize=(30, 30)\n",
    "    )\n",
    "    # Draw a rectangle around the faces\n",
    "    for (x, y, w, h) in faces:\n",
    "        cv2.rectangle(gray, (x, y), (x+w, y+h), (0, 255, 0), 2)\n",
    "\n",
    "\n",
    "    # Display the resulting frame\n",
    "    cv2.imshow('frame',gray)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# When everything done, release the capture\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Capture Video on Camera\n",
    "# Shows video in gray scale with face detection in a new window\n",
    "# Adapted from: https://realpython.com/blog/python/face-recognition-with-python/\n",
    "import numpy as np\n",
    "import cv2\n",
    "import dlib\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "win = dlib.image_window()\n",
    "\n",
    "# Create the haar cascade\n",
    "faceCascade = cv2.CascadeClassifier(\"/usr/local/share/OpenCV/haarcascades/haarcascade_frontalface_default.xml\")\n",
    "\n",
    "# Initialize variables for dLib\n",
    "#predictor_path = \"/Users/pybeebee/HS Science Fair/shape_predictor_68_face_landmarks.dat\"\n",
    "#detector = dlib.get_frontal_face_detector()\n",
    "#predictor = dlib.shape_predictor(predictor_path)\n",
    "\n",
    "\n",
    "while(True):\n",
    "    # Capture frame-by-frame\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    # Detect Face in frame\n",
    "    faces = faceCascade.detectMultiScale(frame, scaleFactor=1.1, minNeighbors=5, minSize=(30, 30))\n",
    "    \n",
    "    # Draw a rectangle around the faces\n",
    "    for (x, y, w, h) in faces:\n",
    "        cv2.rectangle(frame, (x, y), (x+w, y+h), (0, 255, 0), 2)\n",
    "    \n",
    "    # Use dLib to locate and display facial landmarks\n",
    "    #dets = detector(frame, 1)\n",
    "    #for k, d in enumerate(dets):\n",
    "        #shape = predictor(frame, d)\n",
    "        #frame.add_overlay(shape)\n",
    "    \n",
    "    # Display the resulting frame\n",
    "    #cv2.imshow('frame',frame) \n",
    "    win.clear_overlay()\n",
    "    win.set_image(frame)\n",
    "    \n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# When everything done, release the capture\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Show video in color\n",
    "# Displays box around face\n",
    "# Displays facial landmarks for each face\n",
    "\n",
    "import cv2\n",
    "import dlib\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# Initialize variables for dLib\n",
    "predictor_path = \"/Users/kaililiu/HS Science Fair/shape_predictor_68_face_landmarks.dat\"\n",
    "detector = dlib.get_frontal_face_detector()\n",
    "predictor = dlib.shape_predictor(predictor_path)\n",
    "\n",
    "while(True):\n",
    "    # Capture frame-by-frame\n",
    "    ret, frame = cap.read()\n",
    "    \n",
    "    # Detect faces and initialize variables for next step\n",
    "    dets = detector(frame, 1)\n",
    "    x_list = list()\n",
    "    y_list = list()\n",
    "    \n",
    "    # Get list of x-coordinates and y-coordinates of facial landmarks for each face\n",
    "    for d in dets:\n",
    "        shape = predictor(frame, d)\n",
    "        for index in range(0, 68):\n",
    "            point = shape.part(index)\n",
    "            x_list.append(point.x)\n",
    "            y_list.append(point.y)\n",
    "    \n",
    "    # Draw a rectangle around the faces\n",
    "    for rect in dets:\n",
    "        cv2.rectangle(frame, (rect.left(), rect.top()), (rect.right(), rect.bottom()), (0, 255, 0), 2)\n",
    "    \n",
    "    # Display facial landmark points on frame\n",
    "    for i in range((len(dets)*68)):\n",
    "        cv2.circle(frame, (x_list[i], y_list[i]), 3, (0, 0, 255), -1)\n",
    "       \n",
    "    # Display frame, press 'q' to exit video window\n",
    "    cv2.imshow('frame',frame)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# When everything done, release the capture\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Show video in color\n",
    "# DOES NOT display box around face\n",
    "# Displays facial landmarks for each face\n",
    "\n",
    "import cv2\n",
    "import dlib\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# Initialize variables for dLib\n",
    "predictor_path = \"/Users/kaililiu/HS Science Fair/shape_predictor_68_face_landmarks.dat\"\n",
    "detector = dlib.get_frontal_face_detector()\n",
    "predictor = dlib.shape_predictor(predictor_path)\n",
    "\n",
    "while(True):\n",
    "    # Capture frame-by-frame\n",
    "    ret, frame = cap.read()\n",
    "    \n",
    "    # Detect faces and initialize variables for next step\n",
    "    dets = detector(frame, 1)\n",
    "    x_list = list()\n",
    "    y_list = list()\n",
    "    \n",
    "    # Get list of x-coordinates and y-coordinates of facial landmarks for each face\n",
    "    for d in dets:\n",
    "        shape = predictor(frame, d)\n",
    "        for index in range(0, 68):\n",
    "            point = shape.part(index)\n",
    "            x_list.append(point.x)\n",
    "            y_list.append(point.y)\n",
    "    \n",
    "    # Display facial landmark points on frame\n",
    "    for i in range((len(dets)*68)):\n",
    "        cv2.circle(frame, (x_list[i], y_list[i]), 4, (0, 255, 0), -1)\n",
    "       \n",
    "    # Display frame, press 'q' to exit video window\n",
    "    cv2.imshow('frame',frame)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# When everything is done, release the capture\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Show video in color\n",
    "# DOES NOT display box around face\n",
    "# Displays facial landmarks for each face, change display upon pressing 'n' or 'o' keys\n",
    "\n",
    "import numpy as np\n",
    "import cv2\n",
    "import dlib\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "state = 1\n",
    "\n",
    "# Initialize variables for dLib\n",
    "predictor_path = \"/Users/kaililiu/HS Science Fair/shape_predictor_68_face_landmarks.dat\"\n",
    "detector = dlib.get_frontal_face_detector()\n",
    "predictor = dlib.shape_predictor(predictor_path)\n",
    "\n",
    "while(True):\n",
    "    # Capture frame-by-frame\n",
    "    ret, frame = cap.read()\n",
    "    origFrame = frame.copy()\n",
    "    \n",
    "    # Detect faces and initialize variables for next step\n",
    "    dets = detector(frame, 1)\n",
    "    x_list = list()\n",
    "    y_list = list()\n",
    "    \n",
    "    # Get list of x-coordinates and y-coordinates of facial landmarks for each face\n",
    "    for d in dets:\n",
    "        shape = predictor(frame, d)\n",
    "        for index in range(0, 68):\n",
    "            point = shape.part(index)\n",
    "            x_list.append(point.x)\n",
    "            y_list.append(point.y)\n",
    "    \n",
    "    # Display facial landmark points on frame\n",
    "    for i in range((len(dets)*68)):\n",
    "        cv2.circle(frame, (x_list[i], y_list[i]), 3, (0, 255, 0), -1)\n",
    "       \n",
    "    # Display frame, n = display with lm, o = display w/o lm, q = exit video window\n",
    "    #cv2.imshow('frame',frame)\n",
    "    key = cv2.waitKey(5)\n",
    "    if key == ord('o'):\n",
    "        state = 0\n",
    "    elif key == ord('l'):\n",
    "        state = 1\n",
    "    elif key == ord('q'):\n",
    "        break\n",
    "        \n",
    "    if state == 0:\n",
    "        cv2.imshow('frame',origFrame)\n",
    "    if state == 1:\n",
    "        cv2.imshow('frame',frame)\n",
    "        \n",
    "\n",
    "# When everything is done, release the capture\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Classifies images in CK+ dataset to either neutral or expression category folders\n",
    "import os\n",
    "\n",
    "for root, dirs, files in os.walk(\"./cohn-kanade-images\"):\n",
    "    filelist=[]\n",
    "    for name in files:\n",
    "        filename = os.path.join(root, name)\n",
    "        if '.png' in filename:\n",
    "            filelist.append(filename)\n",
    "    if len(filelist) > 0:\n",
    "        neutralfile = filelist[0]\n",
    "        !cp $neutralfile Neutral\n",
    "        expressfile = filelist[-1]\n",
    "        !cp $expressfile Expression\n",
    "        print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Apply 68-point Facial Landmark Detection Model to CK+ Dataset, neutral faces\n",
    "# DOES display box around face\n",
    "# Displays facial landmarks for each face\n",
    "\n",
    "import cv2\n",
    "import dlib\n",
    "import os\n",
    "\n",
    "# Initialize variables for dLib\n",
    "predictor_path = \"/Users/kaililiu/HS Science Fair/shape_predictor_68_face_landmarks.dat\"\n",
    "detector = dlib.get_frontal_face_detector()\n",
    "predictor = dlib.shape_predictor(predictor_path)\n",
    "\n",
    "for root, dirs, files in os.walk(\"./Neutral\"):\n",
    "    for name in files:\n",
    "        filename = os.path.join(root, name)\n",
    "\n",
    "        # Read image if of correct type (8bit gray or RGB)\n",
    "        if '.png' in filename:\n",
    "            image = cv2.imread(filename, 1)\n",
    "\n",
    "            # Detect faces and initialize variables for next step\n",
    "            dets = detector(image, 1)\n",
    "            x_list = list()\n",
    "            y_list = list()\n",
    "\n",
    "            # Get list of x-coordinates and y-coordinates of facial landmarks for each face\n",
    "            for d in dets:\n",
    "                shape = predictor(image, d)\n",
    "                for index in range(0, 68):\n",
    "                    point = shape.part(index)\n",
    "                    x_list.append(point.x)\n",
    "                    y_list.append(point.y)\n",
    "\n",
    "            # Display rectangle around face\n",
    "            for rect in dets:\n",
    "                cv2.rectangle(image, (rect.left(), rect.top()), (rect.right(), rect.bottom()), (0, 255, 0), 2)\n",
    "            \n",
    "            # Display facial landmark points on frame\n",
    "            for i in range((len(dets)*68)):\n",
    "                cv2.circle(image, (x_list[i], y_list[i]), 3, (0, 255, 0), -1)\n",
    "\n",
    "            # Display frame, 'q' to stop\n",
    "            cv2.imshow('frame',image)\n",
    "            key = cv2.waitKey(50)\n",
    "            if key == ord('q'):\n",
    "                break\n",
    "\n",
    "# When everything is done, destroy the window\n",
    "#cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Apply 68-point Facial Landmark Detection Model to CK+ Dataset, faces with expressions\n",
    "# DOES display box around face\n",
    "# Displays facial landmarks for each face\n",
    "\n",
    "import cv2\n",
    "import dlib\n",
    "import os\n",
    "\n",
    "# Initialize variables for dLib\n",
    "predictor_path = \"/Users/kaililiu/HS Science Fair/shape_predictor_68_face_landmarks.dat\"\n",
    "detector = dlib.get_frontal_face_detector()\n",
    "predictor = dlib.shape_predictor(predictor_path)\n",
    "\n",
    "for root, dirs, files in os.walk(\"./Expression\"):\n",
    "    for name in files:\n",
    "        filename = os.path.join(root, name)\n",
    "\n",
    "        # Read image if of correct type (8bit gray or RGB)\n",
    "        if '.png' in filename:\n",
    "            image = cv2.imread(filename, 1)\n",
    "\n",
    "            # Detect faces and initialize variables for next step\n",
    "            dets = detector(image, 1)\n",
    "            x_list = list()\n",
    "            y_list = list()\n",
    "\n",
    "            # Get list of x-coordinates and y-coordinates of facial landmarks for each face\n",
    "            for d in dets:\n",
    "                shape = predictor(image, d)\n",
    "                for index in range(0, 68):\n",
    "                    point = shape.part(index)\n",
    "                    x_list.append(point.x)\n",
    "                    y_list.append(point.y)\n",
    "\n",
    "            # Display rectangle around face\n",
    "            for rect in dets:\n",
    "                cv2.rectangle(image, (rect.left(), rect.top()), (rect.right(), rect.bottom()), (0, 255, 0), 2)\n",
    "            \n",
    "            # Display facial landmark points on frame\n",
    "            for i in range((len(dets)*68)):\n",
    "                cv2.circle(image, (x_list[i], y_list[i]), 3, (0, 255, 0), -1)\n",
    "\n",
    "            # Display frame, 'q' to stop\n",
    "            cv2.imshow('frame',image)\n",
    "            key = cv2.waitKey(50)\n",
    "            if key == ord('q'):\n",
    "                break\n",
    "\n",
    "# When everything is done, destroy the window\n",
    "#cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# OLD: get numpy array of expression and neutral differences for each image\n",
    "import cv2\n",
    "import dlib\n",
    "import os\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "# Initialize variables for dLib\n",
    "predictor_path = \"/Users/kaililiu/HS Science Fair/shape_predictor_68_face_landmarks.dat\"\n",
    "detector = dlib.get_frontal_face_detector()\n",
    "predictor = dlib.shape_predictor(predictor_path)\n",
    "# Initialize other variables\n",
    "NLA = []\n",
    "ELA = []\n",
    "tempLDA = []\n",
    "# Get landmarks for neutral images\n",
    "for root, dirs, files in os.walk(\"./Neutral\"):\n",
    "    for name in files:\n",
    "        filename = os.path.join(root, name)\n",
    "\n",
    "        # Read image if of correct type (8bit gray or RGB)\n",
    "        if '.png' in filename:\n",
    "            image = cv2.imread(filename, 1)\n",
    "\n",
    "            # Detect faces and initialize variables for next step\n",
    "            dets = detector(image, 1)\n",
    "\n",
    "            # Get list of x-coordinates and y-coordinates of facial landmarks for each face\n",
    "            for d in dets:\n",
    "                imageNA = []\n",
    "                shape = predictor(image, d)\n",
    "                for index in range(0, 68):\n",
    "                    point = shape.part(index)\n",
    "                    tuplepoint = (point.x, point.y)\n",
    "                    imageNA.append(tuplepoint)\n",
    "                NLA.append(imageNA)\n",
    "                \n",
    "# Get landmarks for expression images\n",
    "for root, dirs, files in os.walk(\"./Expression\"):\n",
    "    for name in files:\n",
    "        filename = os.path.join(root, name)\n",
    "\n",
    "        # Read image if of correct type (8bit gray or RGB)\n",
    "        if '.png' in filename:\n",
    "            image = cv2.imread(filename, 1)\n",
    "\n",
    "            # Detect faces and initialize variables for next step\n",
    "            dets = detector(image, 1)\n",
    "\n",
    "            # Get list of x-coordinates and y-coordinates of facial landmarks for each face\n",
    "            for d in dets:\n",
    "                imageEA = []\n",
    "                shape = predictor(image, d)\n",
    "                for index in range(0, 68):\n",
    "                    point = shape.part(index)\n",
    "                    tuplepoint = (point.x, point.y)\n",
    "                    imageEA.append(tuplepoint)\n",
    "                ELA.append(imageEA)\n",
    "\n",
    "# Get list of Euclidean differences for each landmark point for each image, convert list to numpy.arrray, add to big list\n",
    "for img in range(len(NLA)):\n",
    "    tempImgLDA = []\n",
    "    for point in range(68):\n",
    "        dif = math.sqrt((math.pow(ELA[img][point][0]-NLA[img][point][0], 2)) + (math.pow(ELA[img][point][1]-NLA[img][point][1], 2)))\n",
    "        tempImgLDA.append(dif)\n",
    "    imgLDA = np.asarray(tempImgLDA)\n",
    "    tempLDA.append(imgLDA)\n",
    "\n",
    "# Convert big list of all differences for all images to numpy.array\n",
    "LDA = np.asarray(tempLDA)\n",
    "\n",
    "# Subtract the average from the LDA values to see if there is any change\n",
    "for img in range(len(LDA)):\n",
    "    sum = 0\n",
    "    for valueIndex in range(len(LDA[img])):\n",
    "        sum+=LDA[img][valueIndex]\n",
    "    average = sum/68\n",
    "    for vI in range(len(LDA[img])):\n",
    "        LDA[img][vI]-=average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create 327x3 matrix, for each image set contains neutral image name, expression image name, emotion label\n",
    "# Generate normalized Euclidean Differences array\n",
    "# Generate target values array\n",
    "# Feed to SVM, evaulate with Cross Validation\n",
    "import cv2\n",
    "import dlib\n",
    "import os\n",
    "import math\n",
    "\n",
    "# Function that returns array of 68 landmarks for given image name\n",
    "def get68LMA(imgName):\n",
    "    predictor_path = \"/Users/kaililiu/HS Science Fair/shape_predictor_68_face_landmarks.dat\"\n",
    "    detector = dlib.get_frontal_face_detector()\n",
    "    predictor = dlib.shape_predictor(predictor_path)\n",
    "    LMA = []\n",
    "    image = cv2.imread(imgName, 1)\n",
    "    \n",
    "    # Detect face(s)\n",
    "    dets = detector(image, 1)\n",
    "\n",
    "    # Get list of coordinates of 68 facial landmarks for each face\n",
    "    for d in dets:\n",
    "        shape = predictor(image, d)\n",
    "        for index in range(0, 68):\n",
    "            point = shape.part(index)\n",
    "            tuplepoint = (point.x, point.y)\n",
    "            LMA.append(tuplepoint)\n",
    "    return LMA\n",
    "\n",
    "# Function that eturns normalized array of Euclidean distances given two arrays of coordinates\n",
    "def getEucDist(nLA, eLA):\n",
    "    LDA = []\n",
    "    sum = 0\n",
    "    # Get array of distances\n",
    "    for pt in range(68):\n",
    "        x1 = eLA[pt][0]\n",
    "        x2 = nLA[pt][0]\n",
    "        y1 = eLA[pt][1]\n",
    "        y2 = nLA[pt][1]\n",
    "        dif = math.sqrt((x1-x2)*(x1-x2) + (y1-y2)*(y1-y2))     \n",
    "        LDA.append(dif)\n",
    "        sum += dif\n",
    "    avg = sum/68\n",
    "    # Normalize array\n",
    "    for i in range(68):\n",
    "        LDA[i] -= avg\n",
    "        \n",
    "    return LDA\n",
    "\n",
    "\n",
    "# Get paths to face images and emotion labels\n",
    "imagedir = []\n",
    "labeldir = []\n",
    "root, dirs, files = os.walk(\"./cohn-kanade-images\",).next()\n",
    "for d in dirs:\n",
    "    imagedir.append(os.path.join(root,d))\n",
    "    labeldir.append(os.path.join('./Emotion',d))\n",
    "\n",
    "# initialize datset\n",
    "dataset = []\n",
    "\n",
    "# look over all directories of individuals S001 through S999\n",
    "for d in dirs:\n",
    "    for dd in os.listdir(os.path.join('./cohn-kanade-images',d)):\n",
    "        # skip over .DS_Store files\n",
    "        if 'Store' not in dd:\n",
    "            curdir = os.path.join('./cohn-kanade-images',d,dd)\n",
    "            files = os.listdir(curdir)\n",
    "            baseimage = os.path.join(curdir,files[0])\n",
    "            faceimage = os.path.join(curdir,files[-1])\n",
    "\n",
    "            # get directories of multiple expressions of same person\n",
    "            labeldir = os.path.join('./Emotion',d,dd)\n",
    "            if os.path.isdir(labeldir):\n",
    "                labels = os.listdir(labeldir)\n",
    "                # read label file if it exists\n",
    "                if len(labels)>0:\n",
    "                    f=open(os.path.join(labeldir,labels[0]),\"r\")\n",
    "                    for line in f:\n",
    "                        targetlabel = float(line)\n",
    "                    f.close()\n",
    "                    \n",
    "                    # append label to dataset\n",
    "                    dataset.append([baseimage, faceimage, targetlabel])\n",
    "\n",
    "# Get array of Euclidean differences between neutral and expression 68-landmark arrays\n",
    "LDA = []\n",
    "for imgIndex in range(len(dataset)):\n",
    "    nImgName = dataset[imgIndex][0]\n",
    "    eImgName = dataset[imgIndex][1]\n",
    "    nLA = get68LMA(nImgName)\n",
    "    eLA = get68LMA(eImgName)\n",
    "    LDA.append(getEucDist(nLA, eLA))\n",
    "\n",
    "# Create target value array for expression images\n",
    "targetValues = []\n",
    "for imgSet in range(len(dataset)):\n",
    "    targetValues.append(dataset[imgSet][2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "realLDA = np.asmatrix(LDA)\n",
    "np.savetxt('LDAfile', realLDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "realTargetValues = np.asarray(targetValues)\n",
    "np.savetxt('targetValuesFile', realTargetValues)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# OLD: use cross validation\n",
    "from sklearn import svm\n",
    "from sklearn import cross_validation\n",
    "clf = svm.SVC(kernel='linear').fit(realLDA, realTargetValues)\n",
    "scores = cross_validation.cross_val_score(clf, realLDA, realTargetValues, cv=4)\n",
    "print scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Calculate confusion matrix for base model with SVM and Cross Validation\n",
    "# Convert confusion matrix to contain percentages \n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "x_train, x_test, y_train, y_test = cross_validation.train_test_split(realLDA, realTargetValues, test_size=0.4, random_state=2)\n",
    "clf2 = svm.SVC(kernel='linear').fit(x_train, y_train)\n",
    "predictions = clf2.predict(x_test)\n",
    "cmatrix = confusion_matrix(y_test, predictions)\n",
    "\n",
    "# Format CMatrix to have percentages\n",
    "np.set_printoptions(precision=1)\n",
    "rowSums = np.sum(cmatrix, axis=1)\n",
    "pCMatrix = np.divide(cmatrix*100., np.sum(cmatrix, axis=1), dtype=float)\n",
    "print pCMatrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Show video in color\n",
    "# Displays box around face\n",
    "# Displays facial landmarks for each face\n",
    "\n",
    "import cv2\n",
    "import dlib\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# Initialize variables for dLib\n",
    "detector = dlib.get_frontal_face_detector()\n",
    "\n",
    "while(True):\n",
    "    # Capture frame-by-frame\n",
    "    ret, frame = cap.read()\n",
    "    \n",
    "    # Detect faces and initialize variables for next step\n",
    "    dets = detector(frame, 1)\n",
    "    \n",
    "       \n",
    "    # Draw a rectangle around the faces\n",
    "    for rect in dets:\n",
    "        cv2.rectangle(frame, (rect.left(), rect.top()), (rect.right(), rect.bottom()), (0, 255, 0), 2)\n",
    "       \n",
    "    # Display frame, press 'q' to exit video window\n",
    "    cv2.imshow('frame',frame)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# When everything done, release the capture\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
